#!/usr/bin/env python3
"""
NEW2 ì‹œë¦¬ì¦ˆ ë°ì´í„°ë¥¼ ì§ì ‘ 3í´ë˜ìŠ¤ë¡œ ë³‘í•©í•˜ê³  6:2:2 ë¶„í• í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ NEW2 ë°ì´í„° íŒŒì¼ë“¤ì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import json
from datetime import datetime

def load_new2_data():
    """NEW2 ì‹œë¦¬ì¦ˆ ë°ì´í„° ë¡œë“œ"""
    
    print("ğŸ“Š NEW2 ì‹œë¦¬ì¦ˆ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # NEW2 íŒŒì¼ ê²½ë¡œ ì •ì˜
    data_files = {
        'kor_quake': ('new2_kor_quake_data.npy', 'new2_kor_quake_labels.npy'),
        'jpn_quake': ('new2_jpn_quake_data.npy', 'new2_jpn_quake_labels.npy'),
        'industry': ('new2_industry_data.npy', 'new2_industry_labels.npy'),
        'motor': ('new2_motor_data.npy', 'new2_motor_labels.npy'),
        'live': ('new2_live_data.npy', 'new2_live_labels.npy'),
        'irregular': ('new2_irregular_data.npy', 'new2_irregular_labels.npy')
    }
    
    # í´ë˜ìŠ¤ ë§¤í•‘ (industry ë°ì´í„° ì œì™¸)
    class_mapping = {
        'kor_quake': 0,    # ì§€ì§„
        'jpn_quake': 0,    # ì§€ì§„
        'motor': 1,        # ê·œì¹™ì  ì‚°ì—…ì§„ë™ (ëª¨í„°ë§Œ)
        'live': 2,         # ë¶ˆê·œì¹™ ìƒí™œì§„ë™
        'irregular': 2     # ë¶ˆê·œì¹™ ìƒí™œì§„ë™
        # 'industry' ì œì™¸ - ëª¨ë“  ë¶„ì„ì´ ì´ í´ë˜ìŠ¤ë¡œ í¸í–¥ë˜ëŠ” ë¬¸ì œ í•´ê²°
    }
    
    all_data = []
    all_labels = []
    all_sources = []
    source_stats = {}
    
    for source, (data_file, label_file) in data_files.items():
        # industry ë°ì´í„°ëŠ” ê±´ë„ˆëœ€ (í¸í–¥ ë¬¸ì œ í•´ê²°)
        if source == 'industry':
            print(f"âš ï¸ {source}: í¸í–¥ ë°©ì§€ë¥¼ ìœ„í•´ ì œì™¸ë¨")
            continue
            
        if os.path.exists(data_file) and os.path.exists(label_file):
            # í´ë˜ìŠ¤ ë§¤í•‘ì— ìˆëŠ” ì†ŒìŠ¤ë§Œ ì²˜ë¦¬
            if source in class_mapping:
                try:
                    # ë°ì´í„° ë¡œë“œ
                    data = np.load(data_file)
                    labels = np.load(label_file)
                    
                    print(f"âœ… {source}: {len(data)}ê°œ ë¡œë“œ - í˜•íƒœ: {data.shape}")
                    
                    # ìƒˆë¡œìš´ í´ë˜ìŠ¤ ë¼ë²¨ í• ë‹¹
                    new_labels = np.full(len(data), class_mapping[source])
                    sources = np.full(len(data), source, dtype='<U20')
                    
                    all_data.append(data)
                    all_labels.append(new_labels)
                    all_sources.append(sources)
                    
                    source_stats[source] = {
                        'count': len(data),
                        'class': class_mapping[source],
                        'shape': data.shape
                    }
                    
                except Exception as e:
                    print(f"âŒ {source} ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                print(f"âš ï¸ {source}: í´ë˜ìŠ¤ ë§¤í•‘ì—ì„œ ì œì™¸ë¨")
        else:
            print(f"âš ï¸ {source}: íŒŒì¼ ì—†ìŒ ({data_file} ë˜ëŠ” {label_file})")
    
    if not all_data:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    # ë°ì´í„° í†µí•©
    combined_data = np.vstack(all_data)
    combined_labels = np.hstack(all_labels)
    combined_sources = np.hstack(all_sources)
    
    print(f"\nğŸ¯ NEW2 ë°ì´í„° í†µí•© ì™„ë£Œ:")
    print(f"   ì „ì²´ ìƒ˜í”Œ: {len(combined_data)}ê°œ")
    print(f"   ë°ì´í„° í˜•íƒœ: {combined_data.shape}")
    
    return combined_data, combined_labels, combined_sources, source_stats

def analyze_class_distribution(labels, sources):
    """3í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
    
    print("\nğŸ“ˆ === 3í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ===")
    
    class_names = {0: 'ì§€ì§„', 1: 'ê·œì¹™ì _ì‚°ì—…ì§„ë™', 2: 'ë¶ˆê·œì¹™_ìƒí™œì§„ë™'}
    
    class_stats = {}
    total_samples = len(labels)
    
    for class_id in range(3):
        mask = labels == class_id
        class_count = np.sum(mask)
        class_sources = sources[mask]
        
        print(f"\ní´ë˜ìŠ¤ {class_id} ({class_names[class_id]}): {class_count}ê°œ ({class_count/total_samples*100:.1f}%)")
        
        # ì†ŒìŠ¤ë³„ ì„¸ë¶€ ë¶„í¬
        unique_sources, counts = np.unique(class_sources, return_counts=True)
        source_breakdown = {}
        
        for source, count in zip(unique_sources, counts):
            print(f"   â”œâ”€â”€ {source}: {count:,}ê°œ")
            source_breakdown[source] = int(count)
        
        class_stats[class_id] = {
            'name': class_names[class_id],
            'total': int(class_count),
            'percentage': round(class_count/total_samples*100, 1),
            'sources': source_breakdown
        }
    
    # ê· í˜•ì„± ë¶„ì„
    class_counts = [class_stats[i]['total'] for i in range(3)]
    min_count = min(class_counts)
    max_count = max(class_counts)
    
    print(f"\nâš–ï¸ í´ë˜ìŠ¤ ê· í˜•ì„±:")
    print(f"   ìµœì†Œ: {min_count:,}ê°œ, ìµœëŒ€: {max_count:,}ê°œ")
    print(f"   ê· í˜• ë¹„ìœ¨: {max_count/min_count:.1f}:1")
    
    if max_count/min_count <= 2.0:
        print("   âœ… ì–‘í˜¸í•œ ê· í˜• (2:1 ì´í•˜)")
    else:
        print("   âš ï¸ ë¶ˆê· í˜• (ê· í˜• ì¡°ì • ê¶Œì¥)")
    
    return class_stats

def create_balanced_dataset(data, labels, sources, method='min'):
    """ê· í˜• ë°ì´í„°ì…‹ ìƒì„±"""
    
    print(f"\nâš–ï¸ === ê· í˜• ë°ì´í„°ì…‹ ìƒì„± ({method}) ===")
    
    if method == 'none':
        return data, labels, sources
    
    class_counts = [np.sum(labels == i) for i in range(3)]
    target_count = min(class_counts) if method == 'min' else max(class_counts)
    
    print(f"ëª©í‘œ ìƒ˜í”Œ ìˆ˜: {target_count:,}ê°œ (í´ë˜ìŠ¤ë‹¹)")
    
    balanced_data = []
    balanced_labels = []
    balanced_sources = []
    
    np.random.seed(42)  # ì¬í˜„ì„± í™•ë³´
    
    for class_id in range(3):
        mask = labels == class_id
        class_indices = np.where(mask)[0]
        
        if len(class_indices) >= target_count:
            # ë‹¤ìš´ìƒ˜í”Œë§
            selected_indices = np.random.choice(class_indices, target_count, replace=False)
            print(f"   í´ë˜ìŠ¤ {class_id}: {target_count:,}ê°œ ì„ íƒ (ë‹¤ìš´ìƒ˜í”Œë§)")
        else:
            # ì˜¤ë²„ìƒ˜í”Œë§
            selected_indices = np.random.choice(class_indices, target_count, replace=True)
            print(f"   í´ë˜ìŠ¤ {class_id}: {target_count:,}ê°œ ì„ íƒ (ì˜¤ë²„ìƒ˜í”Œë§)")
        
        balanced_data.append(data[selected_indices])
        balanced_labels.append(labels[selected_indices])
        balanced_sources.append(sources[selected_indices])
    
    # í†µí•© ë° ì…”í”Œ
    final_data = np.vstack(balanced_data)
    final_labels = np.hstack(balanced_labels)
    final_sources = np.hstack(balanced_sources)
    
    final_data, final_labels, final_sources = shuffle(
        final_data, final_labels, final_sources, random_state=42
    )
    
    print(f"âœ… ê· í˜• ë°ì´í„°ì…‹ ì™„ë£Œ: {len(final_data):,}ê°œ")
    print(f"   ìµœì¢… ë¶„í¬: {np.bincount(final_labels)}")
    
    return final_data, final_labels, final_sources

def split_dataset_622(data, labels, sources):
    """6:2:2 ë¹„ìœ¨ë¡œ ë°ì´í„° ë¶„í• """
    
    print(f"\nğŸ“Š === ë°ì´í„° ë¶„í•  (6:2:2) ===")
    
    # 1ì°¨: ì „ì²´ì—ì„œ í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ (20%)
    X_temp, X_test, y_temp, y_test, src_temp, src_test = train_test_split(
        data, labels, sources,
        test_size=0.2,
        stratify=labels,
        random_state=42
    )
    
    # 2ì°¨: ë‚¨ì€ 80%ì—ì„œ í•™ìŠµ:ê²€ì¦ = 3:1 ë¶„ë¦¬ (60%:20%)
    X_train, X_val, y_train, y_val, src_train, src_val = train_test_split(
        X_temp, y_temp, src_temp,
        test_size=0.25,  # 80%ì˜ 25% = ì „ì²´ì˜ 20%
        stratify=y_temp,
        random_state=42
    )
    
    # ë¶„í•  ê²°ê³¼ ì¶œë ¥
    sets = [
        ('í•™ìŠµ', X_train, y_train),
        ('ê²€ì¦', X_val, y_val),
        ('í…ŒìŠ¤íŠ¸', X_test, y_test)
    ]
    
    total_samples = len(data)
    
    for name, X, y in sets:
        print(f"\n{name} ì„¸íŠ¸: {len(X):,}ê°œ ({len(X)/total_samples*100:.1f}%)")
        class_dist = np.bincount(y)
        for i, count in enumerate(class_dist):
            print(f"   í´ë˜ìŠ¤ {i}: {count:,}ê°œ")
    
    return (X_train, y_train, src_train), (X_val, y_val, src_val), (X_test, y_test, src_test)

def save_datasets(train_set, val_set, test_set, output_dir='./new2_3class_dataset'):
    """ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥"""
    
    print(f"\nğŸ’¾ === ë°ì´í„°ì…‹ ì €ì¥ ===")
    
    os.makedirs(output_dir, exist_ok=True)
    
    sets = [
        ('train', train_set),
        ('val', val_set),
        ('test', test_set)
    ]
    
    saved_files = []
    
    for name, (X, y, src) in sets:
        # íŒŒì¼ ê²½ë¡œ
        data_file = f"new2_3class_{name}_X.npy"
        label_file = f"new2_3class_{name}_y.npy"
        source_file = f"new2_3class_{name}_sources.npy"
        
        # ì €ì¥
        np.save(os.path.join(output_dir, data_file), X)
        np.save(os.path.join(output_dir, label_file), y)
        np.save(os.path.join(output_dir, source_file), src)
        
        saved_files.extend([data_file, label_file, source_file])
        
        print(f"âœ… {name} ì„¸íŠ¸ ì €ì¥:")
        print(f"   ğŸ“ {data_file} - {X.shape}")
        print(f"   ğŸ“ {label_file} - {y.shape}")
        print(f"   ğŸ“ {source_file} - {src.shape}")
    
    return saved_files

def save_metadata(source_stats, class_stats, saved_files, output_dir='./new2_3class_dataset'):
    """ë©”íƒ€ë°ì´í„° ì €ì¥"""
    
    metadata = {
        'creation_info': {
            'timestamp': datetime.now().isoformat(),
            'dataset_name': 'NEW2_3class_balanced',
            'version': 'v2.0',
            'description': 'NEW2 ì‹œë¦¬ì¦ˆ ë°ì´í„°ë¥¼ 3í´ë˜ìŠ¤ë¡œ ë³‘í•©í•œ ê· í˜• ë°ì´í„°ì…‹'
        },
        'data_info': {
            'total_samples': sum(stats['total'] for stats in class_stats.values()),
            'input_shape': '(N, 40, 3, 100, 1)',
            'sampling_rate': '100Hz',
            'duration': '40ì´ˆ (ì „ 15ì´ˆ + í›„ 25ì´ˆ)',
            'normalization': 'z-score'
        },
        'class_mapping': {
            0: 'ì§€ì§„ (êµ­ë‚´ì§€ì§„ + ì¼ë³¸ì§€ì§„)',
            1: 'ê·œì¹™ì  ì‚°ì—…ì§„ë™ (Motorë§Œ, Industry ì œì™¸)',
            2: 'ë¶ˆê·œì¹™ ìƒí™œì§„ë™ (Live + Irregular)'
        },
        'source_statistics': source_stats,
        'class_statistics': class_stats,
        'split_info': {
            'method': 'stratified',
            'ratios': {'train': 0.6, 'val': 0.2, 'test': 0.2},
            'random_state': 42
        },
        'files': {
            'saved_files': saved_files,
            'location': output_dir
        },
        'new2_improvements': {
            'vs_6class': 'í‰ê·  3.5ë°° ë°ì´í„° ì¦ê°€',
            'korean_earthquake': '12ë°° ì¦ê°€ (189ê°œ â†’ 2,308ê°œ)',
            'success_rate': '100% (ì‹¤íŒ¨ íŒŒì¼ ì—†ìŒ)',
            'convlstm_compatibility': 'ì™„ë²½í•œ í˜¸í™˜ì„±'
        }
    }
    
    # JSON ì €ì¥
    metadata_file = 'new2_3class_metadata.json'
    with open(os.path.join(output_dir, metadata_file), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")
    
    return metadata

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¯ NEW2 ì‹œë¦¬ì¦ˆ 3í´ë˜ìŠ¤ ë³‘í•© ë° ë¶„í• ")
    print("=" * 60)
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        data, labels, sources, source_stats = load_new2_data()
        
        # 2. ë¶„í¬ ë¶„ì„
        class_stats = analyze_class_distribution(labels, sources)
        
        # 3. ê· í˜• ì¡°ì • (ì„ íƒ)
        print("\nğŸ¤” ê· í˜• ì¡°ì • ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("   1. ìµœì†Œê°’ ê¸°ì¤€ ê· í˜• ì¡°ì • (ê¶Œì¥)")
        print("   2. ê· í˜• ì¡°ì • ì—†ìŒ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)")
        
        choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()
        
        if choice == '2':
            balance_method = 'none'
            balanced_data, balanced_labels, balanced_sources = data, labels, sources
        else:
            balance_method = 'min'
            balanced_data, balanced_labels, balanced_sources = create_balanced_dataset(
                data, labels, sources, method='min'
            )
        
        # 4. 6:2:2 ë¶„í• 
        train_set, val_set, test_set = split_dataset_622(
            balanced_data, balanced_labels, balanced_sources
        )
        
        # 5. ì €ì¥
        saved_files = save_datasets(train_set, val_set, test_set)
        
        # 6. ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = save_metadata(source_stats, class_stats, saved_files)
        
        print(f"\nğŸ‰ === NEW2 3í´ë˜ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ! ===")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: ./new2_3class_dataset/")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ: {len(balanced_data):,}ê°œ")
        print(f"ğŸ¯ í´ë˜ìŠ¤ ìˆ˜: 3ê°œ (ê· í˜• ì¡°ì •: {'ì˜ˆ' if balance_method != 'none' else 'ì•„ë‹ˆì˜¤'})")
        print(f"ğŸ“ˆ ë¶„í•  ë¹„ìœ¨: í•™ìŠµ 60% : ê²€ì¦ 20% : í…ŒìŠ¤íŠ¸ 20%")
        
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. ConvLSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"   2. ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ")
        print(f"   3. ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")
    else:
        print("\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨!")