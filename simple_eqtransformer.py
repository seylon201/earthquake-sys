
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv1D, LSTM, Dense, Dropout, BatchNormalization,
    GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization,
    Add, Activation
)

class SimpleEQTransformer:
    """EQTransformer ëŒ€ì•ˆ ëª¨ë¸ (Transformer-inspired)"""
    
    def __init__(self, input_shape=(6000, 3), num_classes=3):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
    
    def create_model(self):
        """Transformer-inspired ëª¨ë¸ ìƒì„±"""
        inputs = Input(shape=self.input_shape, name='input_waveform')
        
        # 1D CNNìœ¼ë¡œ ì§€ì—­ íŠ¹ì§• ì¶”ì¶œ
        x = Conv1D(64, 15, activation='relu', padding='same')(inputs)
        x = BatchNormalization()(x)
        x = Conv1D(64, 15, activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        
        # ë‹¤ìš´ìƒ˜í”Œë§ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
        x = Conv1D(128, 5, strides=4, activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        x = Conv1D(128, 5, strides=4, activation='relu', padding='same')(x)
        x = BatchNormalization()(x)
        
        # LSTMìœ¼ë¡œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
        x = LSTM(256, return_sequences=True, dropout=0.3)(x)
        x = LSTM(128, return_sequences=True, dropout=0.3)(x)
        
        # Self-Attention (Transformerì˜ í•µì‹¬)
        attention_output = MultiHeadAttention(
            num_heads=8, 
            key_dim=64,
            name='self_attention'
        )(x, x)
        
        # Residual connection
        x = Add()([x, attention_output])
        x = LayerNormalization()(x)
        
        # ì¶”ê°€ ì²˜ë¦¬ì¸µ
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.3)(x)
        
        # Global pooling
        x = GlobalAveragePooling1D()(x)
        
        # ìµœì¢… ë¶„ë¥˜ì¸µ
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.5)(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        
        # ì¶œë ¥ì¸µ
        outputs = Dense(self.num_classes, activation='softmax', name='classification')(x)
        
        # ëª¨ë¸ ìƒì„±
        self.model = Model(inputs=inputs, outputs=outputs, name='SimpleEQTransformer')
        
        return self.model
    
    def compile_model(self, learning_rate=1e-4):
        """ëª¨ë¸ ì»´íŒŒì¼"""
        if self.model is None:
            self.create_model()
        
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)
        
        self.model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return self.model

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
    eqt_alternative = SimpleEQTransformer(input_shape=(6000, 3), num_classes=3)
    model = eqt_alternative.compile_model()
    
    print("âœ… EQTransformer ëŒ€ì•ˆ ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“Š ëª¨ë¸ êµ¬ì¡°: {model.count_params():,} íŒŒë¼ë¯¸í„°")
    model.summary()
