import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import glob
import os
from pathlib import Path

class EarthquakeDataIntegrator:
    def __init__(self):
        self.existing_data = None
        self.cesmd_data = None
        self.integrated_data = None
        
    def load_existing_data(self):
        """
        ê¸°ì¡´ ì§€ì§„ í”„ë¡œì íŠ¸ ë°ì´í„° ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
        """
        print("ğŸ“‚ ê¸°ì¡´ ì§€ì§„ í”„ë¡œì íŠ¸ ë°ì´í„° ë¶„ì„...")
        
        # ê¸°ì¡´ ë°ì´í„° êµ¬ì¡° (í”„ë¡œì íŠ¸ ë¬¸ì„œ ê¸°ë°˜)
        existing_summary = {
            'total_events': 3430,
            'sources': {
                'japan_ohtashi': 1170,
                'korea_kma': 2260
            },
            'magnitude_range': '3.0+',
            'regions': ['Japan', 'Korea'],
            'data_format': '40ì´ˆ ìŠ¬ë¼ì´ì‹±, 3ì¶• ê°€ì†ë„',
            'preprocessing': 'Z-score ì •ê·œí™”, (40, 3, 100, 1) í…ì„œ'
        }
        
        print("ğŸ“Š ê¸°ì¡´ ë°ì´í„° í˜„í™©:")
        print(f"   ì´ ì´ë²¤íŠ¸: {existing_summary['total_events']:,}ê°œ")
        print(f"   ì¼ë³¸ ì˜¤íƒ€ì‹œ: {existing_summary['sources']['japan_ohtashi']:,}ê°œ")
        print(f"   í•œêµ­ KMA: {existing_summary['sources']['korea_kma']:,}ê°œ")
        print(f"   ì§„ë„ ë²”ìœ„: {existing_summary['magnitude_range']}")
        print(f"   ì§€ì—­: {', '.join(existing_summary['regions'])}")
        
        return existing_summary
    
    def load_cesmd_data(self):
        """
        ìƒˆë¡œ ìˆ˜ì§‘ëœ CESMD ë°ì´í„° ë¡œë“œ
        """
        print("\nğŸ“‚ CESMD ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ê°€ì¥ ìµœì‹  CESMD íŒŒì¼ ì°¾ê¸°
        cesmd_files = glob.glob("earthquake_project_expanded_*.csv")
        
        if not cesmd_files:
            print("âŒ CESMD ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¨¼ì € cesmd_expanded_collector.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
        latest_file = max(cesmd_files, key=os.path.getctime)
        print(f"ğŸ“ ë¡œë“œí•  íŒŒì¼: {latest_file}")
        
        try:
            self.cesmd_data = pd.read_csv(latest_file)
            print(f"âœ… CESMD ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.cesmd_data)}ê°œ ì´ë²¤íŠ¸")
            
            # ê¸°ë³¸ í†µê³„
            print(f"ğŸ“Š CESMD ë°ì´í„° ìš”ì•½:")
            print(f"   ì§„ë„ ë²”ìœ„: {self.cesmd_data['mag'].min():.1f} ~ {self.cesmd_data['mag'].max():.1f}")
            print(f"   í‰ê·  ì§„ë„: {self.cesmd_data['mag'].mean():.2f}")
            print(f"   ë°ì´í„° ì»¬ëŸ¼: {len(self.cesmd_data.columns)}ê°œ")
            
            return self.cesmd_data
            
        except Exception as e:
            print(f"âŒ CESMD ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_data_compatibility(self):
        """
        ê¸°ì¡´ ë°ì´í„°ì™€ CESMD ë°ì´í„°ì˜ í˜¸í™˜ì„± ë¶„ì„
        """
        if self.cesmd_data is None:
            print("âŒ CESMD ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ” ë°ì´í„° í˜¸í™˜ì„± ë¶„ì„")
        print("="*50)
        
        # ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„
        print("ğŸ“‹ CESMD ë°ì´í„° ì»¬ëŸ¼ êµ¬ì¡°:")
        for i, col in enumerate(self.cesmd_data.columns):
            print(f"   {i+1:2d}. {col}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['mag', 'time', 'latitude', 'longitude', 'place', 'net']
        missing_columns = []
        present_columns = []
        
        for col in required_columns:
            if col in self.cesmd_data.columns:
                present_columns.append(col)
            else:
                missing_columns.append(col)
        
        print(f"\nâœ… ë³´ìœ  í•„ìˆ˜ ì»¬ëŸ¼: {present_columns}")
        if missing_columns:
            print(f"âŒ ëˆ„ë½ í•„ìˆ˜ ì»¬ëŸ¼: {missing_columns}")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        print(f"\nğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬:")
        
        # ê²°ì¸¡ê°’ í™•ì¸
        missing_data = self.cesmd_data.isnull().sum()
        total_missing = missing_data.sum()
        
        if total_missing > 0:
            print(f"âš ï¸  ê²°ì¸¡ê°’ ë°œê²¬: {total_missing}ê°œ")
            print("   ì£¼ìš” ê²°ì¸¡ê°’:")
            for col, missing in missing_data[missing_data > 0].head().items():
                percentage = (missing / len(self.cesmd_data)) * 100
                print(f"     {col}: {missing}ê°œ ({percentage:.1f}%)")
        else:
            print("âœ… ê²°ì¸¡ê°’ ì—†ìŒ")
        
        # ì§„ë„ ë¶„í¬ ë¶„ì„
        if 'mag' in self.cesmd_data.columns:
            print(f"\nğŸ“Š ì§„ë„ ë¶„í¬ ë¶„ì„:")
            mag_stats = self.cesmd_data['mag'].describe()
            print(f"   ìµœì†Œê°’: {mag_stats['min']:.1f}")
            print(f"   25%: {mag_stats['25%']:.1f}")
            print(f"   ì¤‘ê°„ê°’: {mag_stats['50%']:.1f}")
            print(f"   75%: {mag_stats['75%']:.1f}")
            print(f"   ìµœëŒ€ê°’: {mag_stats['max']:.1f}")
            
            # ì§„ë„ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜
            mag_categories = pd.cut(self.cesmd_data['mag'], 
                                  bins=[3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0], 
                                  labels=['3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0', '5.0-5.5', '5.5-6.0'])
            print(f"\n   ì§„ë„ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            for category, count in mag_categories.value_counts().sort_index().items():
                print(f"     {category}: {count}ê°œ")
        
        return self.cesmd_data
    
    def create_integrated_dataset(self, existing_summary):
        """
        ê¸°ì¡´ ë°ì´í„°ì™€ CESMD ë°ì´í„° í†µí•©
        """
        if self.cesmd_data is None:
            print("âŒ í†µí•©í•  CESMD ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print("\nğŸ”„ ë°ì´í„° í†µí•© ì‘ì—… ì‹œì‘")
        print("="*50)
        
        # CESMD ë°ì´í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš© (ì‹¤ì œ ë°ì´í„° ìˆìŒ)
        integrated = self.cesmd_data.copy()
        
        # ê¸°ì¡´ ë°ì´í„° í˜¸í™˜ì„ ìœ„í•œ ì»¬ëŸ¼ ì¶”ê°€/ìˆ˜ì •
        print("ğŸ”§ ë°ì´í„° í‘œì¤€í™” ì¤‘...")
        
        # 1. ì§€ì—­ ì •ë³´ í‘œì¤€í™”
        if 'country' in integrated.columns:
            # ê¸°ì¡´ í”„ë¡œì íŠ¸ì™€ í˜¸í™˜ë˜ëŠ” ì§€ì—­ ì½”ë“œ ìƒì„±
            region_mapping = {
                'US': 'North America',
                'Mexico': 'North America', 
                'Japan': 'East Asia',
                'Korea': 'East Asia',
                'South Pacific Ocean': 'Pacific',
                'North Pacific Ocean': 'Pacific',
                'Philippine Sea': 'Pacific'
            }
            
            integrated['region'] = integrated['country'].map(region_mapping).fillna('Other')
            integrated['region_code'] = integrated['region'].map({
                'East Asia': 0,      # ê¸°ì¡´ ë°ì´í„° (í•œêµ­, ì¼ë³¸)
                'North America': 1,  # ìƒˆë¡œìš´ ë°ì´í„° (ë¯¸êµ­, ë©•ì‹œì½”)
                'Pacific': 2,        # í•´ì–‘ ì§€ì§„
                'Other': 3
            })
        
        # 2. ë°ì´í„° ì†ŒìŠ¤ í‘œì‹œ
        integrated['data_source'] = 'CESMD'
        integrated['is_new_data'] = True
        
        # 3. ì‹œê°„ ì •ë³´ í‘œì¤€í™”
        if 'time' in integrated.columns:
            try:
                integrated['datetime'] = pd.to_datetime(integrated['time'])
                integrated['year'] = integrated['datetime'].dt.year
                integrated['month'] = integrated['datetime'].dt.month
                integrated['day_of_year'] = integrated['datetime'].dt.dayofyear
                integrated['hour'] = integrated['datetime'].dt.hour
            except Exception as e:
                print(f"âš ï¸ ì‹œê°„ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # 4. ê¸°ì¡´ í”„ë¡œì íŠ¸ í˜¸í™˜ ì»¬ëŸ¼ ì¶”ê°€
        integrated['class_label'] = 0  # ëª¨ë“  ë°ì´í„°ê°€ ì§€ì§„ (ê¸°ì¡´ í”„ë¡œì íŠ¸ ê¸°ì¤€)
        integrated['event_type'] = 'earthquake'
        
        # 5. í†µí•© ë©”íƒ€ë°ì´í„° ì¶”ê°€
        integrated['integration_timestamp'] = datetime.now().isoformat()
        integrated['total_dataset_size'] = existing_summary['total_events'] + len(integrated)
        
        self.integrated_data = integrated
        
        print(f"âœ… ë°ì´í„° í†µí•© ì™„ë£Œ")
        print(f"   CESMD ë°ì´í„°: {len(integrated)}ê°œ")
        print(f"   ê¸°ì¡´ ë°ì´í„°: {existing_summary['total_events']}ê°œ")
        print(f"   í†µí•© ì´ê³„: {existing_summary['total_events'] + len(integrated)}ê°œ")
        
        return integrated
    
    def create_comprehensive_analysis(self):
        """
        í†µí•© ë°ì´í„°ì˜ ì¢…í•© ë¶„ì„
        """
        if self.integrated_data is None:
            print("âŒ ë¶„ì„í•  í†µí•© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ“Š í†µí•© ë°ì´í„° ì¢…í•© ë¶„ì„")
        print("="*50)
        
        df = self.integrated_data
        
        # 1. ê¸°ë³¸ í†µê³„
        print("ğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        print(f"   ì´ ë ˆì½”ë“œ: {len(df):,}ê°œ")
        print(f"   ì´ ì»¬ëŸ¼: {len(df.columns)}ê°œ")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        
        # 2. ì§€ì—­ë³„ ë¶„í¬
        if 'region' in df.columns:
            print(f"\nğŸŒ ì§€ì—­ë³„ ë¶„í¬:")
            region_counts = df['region'].value_counts()
            for region, count in region_counts.items():
                percentage = (count / len(df)) * 100
                print(f"   {region}: {count:,}ê°œ ({percentage:.1f}%)")
        
        # 3. ë„¤íŠ¸ì›Œí¬ë³„ ë¶„í¬
        if 'net' in df.columns:
            print(f"\nğŸŒ ìƒìœ„ ë„¤íŠ¸ì›Œí¬:")
            network_counts = df['net'].value_counts().head(8)
            for network, count in network_counts.items():
                percentage = (count / len(df)) * 100
                print(f"   {network}: {count:,}ê°œ ({percentage:.1f}%)")
        
        # 4. ì‹œê°„ì  ë¶„í¬
        if 'year' in df.columns:
            print(f"\nğŸ“… ì—°ë„ë³„ ë¶„í¬ (ìƒìœ„ 10ë…„):")
            year_counts = df['year'].value_counts().sort_index().tail(10)
            for year, count in year_counts.items():
                print(f"   {year}: {count:,}ê°œ")
        
        # 5. ì§„ë„ ë¶„í¬ ìƒì„¸ ë¶„ì„
        if 'mag' in df.columns:
            print(f"\nâš¡ ì§„ë„ ë¶„í¬ ë¶„ì„:")
            print(f"   ë²”ìœ„: {df['mag'].min():.1f} ~ {df['mag'].max():.1f}")
            print(f"   í‰ê· : {df['mag'].mean():.2f}")
            print(f"   í‘œì¤€í¸ì°¨: {df['mag'].std():.2f}")
            
            # ì§„ë„ë³„ êµ¬ê°„ ë¶„ì„
            mag_bins = [3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 10.0]
            mag_labels = ['3.0-3.4', '3.5-3.9', '4.0-4.4', '4.5-4.9', '5.0-5.4', '5.5-5.9', '6.0+']
            
            df['mag_category'] = pd.cut(df['mag'], bins=mag_bins, labels=mag_labels, include_lowest=True)
            mag_dist = df['mag_category'].value_counts().sort_index()
            
            print(f"   ì§„ë„ êµ¬ê°„ë³„ ë¶„í¬:")
            for category, count in mag_dist.items():
                percentage = (count / len(df)) * 100
                print(f"     {category}: {count:,}ê°œ ({percentage:.1f}%)")
        
        return df
    
    def create_visualizations(self):
        """
        í†µí•© ë°ì´í„° ì‹œê°í™”
        """
        if self.integrated_data is None:
            print("âŒ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ“Š ë°ì´í„° ì‹œê°í™” ìƒì„± ì¤‘...")
        
        df = self.integrated_data
        
        # ì‹œê°í™” ì„¤ì •
        plt.style.use('default')
        fig = plt.figure(figsize=(20, 15))
        
        # 2x3 ê·¸ë¦¬ë“œ ìƒì„±
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. ì§„ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        ax1 = fig.add_subplot(gs[0, 0])
        if 'mag' in df.columns:
            ax1.hist(df['mag'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
            ax1.set_title('ì§„ë„ ë¶„í¬', fontsize=14, fontweight='bold')
            ax1.set_xlabel('ì§„ë„ (Magnitude)')
            ax1.set_ylabel('ë¹ˆë„')
            ax1.grid(True, alpha=0.3)
        
        # 2. ì§€ì—­ë³„ ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)
        ax2 = fig.add_subplot(gs[0, 1])
        if 'region' in df.columns:
            region_counts = df['region'].value_counts()
            colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
            ax2.pie(region_counts.values, labels=region_counts.index, autopct='%1.1f%%', 
                   colors=colors[:len(region_counts)])
            ax2.set_title('ì§€ì—­ë³„ ë¶„í¬', fontsize=14, fontweight='bold')
        
        # 3. ë„¤íŠ¸ì›Œí¬ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ)
        ax3 = fig.add_subplot(gs[0, 2])
        if 'net' in df.columns:
            network_counts = df['net'].value_counts().head(10)
            bars = ax3.bar(range(len(network_counts)), network_counts.values, color='lightcoral')
            ax3.set_title('ìƒìœ„ ë„¤íŠ¸ì›Œí¬ë³„ ë¶„í¬', fontsize=14, fontweight='bold')
            ax3.set_xlabel('ë„¤íŠ¸ì›Œí¬')
            ax3.set_ylabel('ì´ë²¤íŠ¸ ìˆ˜')
            ax3.set_xticks(range(len(network_counts)))
            ax3.set_xticklabels(network_counts.index, rotation=45)
            
            # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 5,
                        f'{int(height)}', ha='center', va='bottom')
        
        # 4. ì—°ë„ë³„ ì¶”ì„¸ (ìµœê·¼ 20ë…„)
        ax4 = fig.add_subplot(gs[1, :])
        if 'year' in df.columns:
            recent_years = df[df['year'] >= 2000]
            yearly_counts = recent_years['year'].value_counts().sort_index()
            
            ax4.plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2, markersize=6)
            ax4.fill_between(yearly_counts.index, yearly_counts.values, alpha=0.3)
            ax4.set_title('ì—°ë„ë³„ ì§€ì§„ ë°œìƒ ì¶”ì„¸ (2000ë…„ ì´í›„)', fontsize=14, fontweight='bold')
            ax4.set_xlabel('ì—°ë„')
            ax4.set_ylabel('ì§€ì§„ ë°œìƒ íšŸìˆ˜')
            ax4.grid(True, alpha=0.3)
        
        # 5. ì§€ë¦¬ì  ë¶„í¬ (ìœ„ë„-ê²½ë„ ì‚°ì ë„)
        ax5 = fig.add_subplot(gs[2, :2])
        if 'latitude' in df.columns and 'longitude' in df.columns:
            scatter = ax5.scatter(df['longitude'], df['latitude'], 
                                c=df['mag'], cmap='Reds', alpha=0.6, s=20)
            ax5.set_title('ì§€ì§„ ë°œìƒ ìœ„ì¹˜ (ì§„ë„ë³„ ìƒ‰ìƒ)', fontsize=14, fontweight='bold')
            ax5.set_xlabel('ê²½ë„')
            ax5.set_ylabel('ìœ„ë„')
            
            # ì»¬ëŸ¬ë°” ì¶”ê°€
            cbar = plt.colorbar(scatter, ax=ax5)
            cbar.set_label('ì§„ë„')
        
        # 6. ì§„ë„ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        ax6 = fig.add_subplot(gs[2, 2])
        if 'mag_category' in df.columns:
            category_counts = df['mag_category'].value_counts().sort_index()
            bars = ax6.bar(range(len(category_counts)), category_counts.values, 
                          color='gold', edgecolor='black')
            ax6.set_title('ì§„ë„ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬', fontsize=14, fontweight='bold')
            ax6.set_xlabel('ì§„ë„ ë²”ìœ„')
            ax6.set_ylabel('ì´ë²¤íŠ¸ ìˆ˜')
            ax6.set_xticks(range(len(category_counts)))
            ax6.set_xticklabels(category_counts.index, rotation=45)
            
            # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax6.text(bar.get_x() + bar.get_width()/2., height + 2,
                        f'{int(height)}', ha='center', va='bottom')
        
        plt.suptitle('í†µí•© ì§€ì§„ ë°ì´í„°ì…‹ ì¢…í•© ë¶„ì„', fontsize=18, fontweight='bold', y=0.98)
        
        # ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"integrated_earthquake_analysis_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š í†µí•© ë¶„ì„ ì‹œê°í™” ì €ì¥: {filename}")
        
        plt.show()
        return filename
    
    def save_integrated_dataset(self, existing_summary):
        """
        í†µí•© ë°ì´í„°ì…‹ ìµœì¢… ì €ì¥
        """
        if self.integrated_data is None:
            print("âŒ ì €ì¥í•  í†µí•© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print("\nğŸ’¾ í†µí•© ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ë©”ì¸ ë°ì´í„°ì…‹ ì €ì¥
        main_filename = f"integrated_earthquake_dataset_{timestamp}.csv"
        self.integrated_data.to_csv(main_filename, index=False)
        print(f"ğŸ“ í†µí•© ë°ì´í„°ì…‹ ì €ì¥: {main_filename}")
        
        # 2. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary_report = {
            'integration_info': {
                'timestamp': timestamp,
                'total_records': len(self.integrated_data),
                'total_columns': len(self.integrated_data.columns)
            },
            'data_sources': {
                'existing_data': {
                    'count': existing_summary['total_events'],
                    'sources': existing_summary['sources']
                },
                'cesmd_data': {
                    'count': len(self.integrated_data),
                    'magnitude_range': f"{self.integrated_data['mag'].min():.1f}-{self.integrated_data['mag'].max():.1f}",
                    'regions': self.integrated_data['region'].value_counts().to_dict() if 'region' in self.integrated_data.columns else {}
                }
            },
            'combined_totals': {
                'total_events': existing_summary['total_events'] + len(self.integrated_data),
                'magnitude_coverage': '3.0-6.0+',
                'geographic_coverage': ['East Asia', 'North America', 'Pacific'],
                'time_span': f"{self.integrated_data['year'].min()}-{self.integrated_data['year'].max()}" if 'year' in self.integrated_data.columns else "Unknown"
            },
            'quality_metrics': {
                'missing_values': self.integrated_data.isnull().sum().sum(),
                'data_completeness': f"{(1 - self.integrated_data.isnull().sum().sum() / (len(self.integrated_data) * len(self.integrated_data.columns))) * 100:.1f}%"
            }
        }
        
        summary_filename = f"integration_report_{timestamp}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, indent=2, ensure_ascii=False, default=str)
        print(f"ğŸ“‹ í†µí•© ë¦¬í¬íŠ¸ ì €ì¥: {summary_filename}")
        
        # 3. íŒŒì¼ í¬ê¸° ë° ìƒì„¸ ì •ë³´
        file_size = os.path.getsize(main_filename)
        print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        print(f"   íŒŒì¼ í¬ê¸°: {file_size / 1024 / 1024:.2f} MB")
        print(f"   ë ˆì½”ë“œ ìˆ˜: {len(self.integrated_data):,}ê°œ")
        print(f"   ì»¬ëŸ¼ ìˆ˜: {len(self.integrated_data.columns)}ê°œ")
        print(f"   ë°ì´í„° ì™„ì„±ë„: {summary_report['quality_metrics']['data_completeness']}")
        
        return main_filename, summary_filename, summary_report

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    print("ğŸš€ ì§€ì§„ ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹œì‘")
    print("="*60)
    
    integrator = EarthquakeDataIntegrator()
    
    # 1. ê¸°ì¡´ ë°ì´í„° í˜„í™© í™•ì¸
    existing_summary = integrator.load_existing_data()
    
    # 2. CESMD ë°ì´í„° ë¡œë“œ
    cesmd_data = integrator.load_cesmd_data()
    
    if cesmd_data is None:
        print("âŒ CESMD ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. ë°ì´í„° í˜¸í™˜ì„± ë¶„ì„
    integrator.analyze_data_compatibility()
    
    # 4. ë°ì´í„° í†µí•©
    integrated_data = integrator.create_integrated_dataset(existing_summary)
    
    if integrated_data is None:
        print("âŒ ë°ì´í„° í†µí•©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 5. ì¢…í•© ë¶„ì„
    integrator.create_comprehensive_analysis()
    
    # 6. ì‹œê°í™” ìƒì„±
    plot_filename = integrator.create_visualizations()
    
    # 7. ìµœì¢… ì €ì¥
    main_file, summary_file, report = integrator.save_integrated_dataset(existing_summary)
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ‰ ë°ì´í„° í†µí•© ì™„ë£Œ!")
    print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"   - í†µí•© ë°ì´í„°ì…‹: {main_file}")
    print(f"   - í†µí•© ë¦¬í¬íŠ¸: {summary_file}")
    print(f"   - ë¶„ì„ ì‹œê°í™”: {plot_filename}")
    
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   ê¸°ì¡´ ë°ì´í„°: {existing_summary['total_events']:,}ê°œ")
    print(f"   CESMD ë°ì´í„°: {len(integrated_data):,}ê°œ") 
    print(f"   ì´ ë°ì´í„°: {report['combined_totals']['total_events']:,}ê°œ")
    print(f"   ì¦ê°€ìœ¨: {((len(integrated_data) / existing_summary['total_events']) * 100):.1f}%")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. {main_file}ì„ ConvLSTM ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©")
    print(f"   2. ì§€ì—­ë³„/ì§„ë„ë³„ ì„±ëŠ¥ ê²€ì¦ ìˆ˜í–‰")
    print(f"   3. ê¸°ì¡´ 98% â†’ 99%+ ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ")

if __name__ == "__main__":
    main()