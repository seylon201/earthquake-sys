import time
from datetime import datetime, timedelta
import requests
from influxdb_client import InfluxDBClient
from urllib.parse import quote
import os
import csv
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model

# âœ… 4í´ë˜ìŠ¤ ConvLSTM ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = 'C:/earthquake_project/convlstm_4class_model.h5'

# âœ… ì‹¤ì‹œê°„ CSV ì €ì¥ ìœ„ì¹˜
OUTPUT_DIR = "C:/earthquake_modeling/earthquake_project_v3/influxLogs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# âœ… 4í´ë˜ìŠ¤ ConvLSTM ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
print("ğŸ”„ 4í´ë˜ìŠ¤ ConvLSTM ëª¨ë¸ ë¡œë”© ì¤‘...")
convlstm_4class_model = load_model(MODEL_PATH)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# ğŸ” ëª¨ë¸ ì •ë³´ í™•ì¸
print(f"ğŸ” ëª¨ë¸ ì…ë ¥ shape: {convlstm_4class_model.input_shape}")
print(f"ğŸ” ëª¨ë¸ ì¶œë ¥ shape: {convlstm_4class_model.output_shape}")

# âœ… í´ë˜ìŠ¤ ì •ì˜
CLASS_NAMES = {
    0: 'ì§€ì§„',
    1: 'ì‚°ì—…ì§„ë™', 
    2: 'ë¶ˆê·œì¹™ìƒí™œ',
    3: 'ëª¨í„°ì§„ë™'
}

CLASS_COLORS = {
    0: 'ğŸ”´',  # ì§€ì§„ - ë¹¨ê°•
    1: 'ğŸ”µ',  # ì‚°ì—…ì§„ë™ - íŒŒë‘  
    2: 'ğŸŸ¢',  # ë¶ˆê·œì¹™ìƒí™œ - ì´ˆë¡
    3: 'ğŸŸ '   # ëª¨í„°ì§„ë™ - ì£¼í™©
}

# âœ… ê°œì„ ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜
def improved_preprocessing(csv_path):
    """
    ê°œì„ ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ - ë°˜ë³µ íŒ¨ë”© ë° ì¶•ë³„ ì •ê·œí™” ì ìš©
    """
    try:
        df = pd.read_csv(csv_path)
        
        # sensor1_x, y, z ì»¬ëŸ¼ í™•ì¸
        required_cols = ['sensor1_x', 'sensor1_y', 'sensor1_z']
        if not all(col in df.columns for col in required_cols):
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {required_cols}")
            return None, None
            
        # 3ì¶• ë°ì´í„° ì¶”ì¶œ
        sensor_data = df[required_cols].values.astype(float)
        original_length = len(sensor_data)
        
        # 4000 ìƒ˜í”Œë¡œ ë§ì¶”ê¸° (40ì´ˆ Ã— 100Hz)
        target_samples = 4000
        
        if len(sensor_data) < target_samples:
            # ğŸ”§ ê°œì„ : ë‹¨ìˆœ 0 íŒ¨ë”© ëŒ€ì‹  ë°˜ë³µ íŒ¨ë”©
            if len(sensor_data) >= 100:  # ìµœì†Œ 1ì´ˆ ì´ìƒ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
                repeat_count = target_samples // len(sensor_data) + 1
                sensor_data = np.tile(sensor_data, (repeat_count, 1))[:target_samples]
                print(f"ğŸ”„ ë°˜ë³µ íŒ¨ë”© ì ìš©: {original_length} â†’ {target_samples}")
            else:
                # ë„ˆë¬´ ì ì€ ë°ì´í„°ëŠ” 0 íŒ¨ë”©
                pad_width = ((0, target_samples - len(sensor_data)), (0, 0))
                sensor_data = np.pad(sensor_data, pad_width, mode='constant')
                print(f"âš ï¸ ì œë¡œ íŒ¨ë”© ì ìš©: {original_length} â†’ {target_samples} (ë°ì´í„° ë¶€ì¡±)")
        elif len(sensor_data) > target_samples:
            # ë„˜ì¹˜ë©´ ìë¥´ê¸°
            sensor_data = sensor_data[:target_samples]
            
        # (4000, 3) â†’ (40, 3, 100, 1) ë³€í™˜
        reshaped = sensor_data.reshape(40, 100, 3)  # 40í”„ë ˆì„, ê° 100ìƒ˜í”Œ, 3ì¶•
        reshaped = np.transpose(reshaped, (0, 2, 1))  # (40, 3, 100)
        reshaped = np.expand_dims(reshaped, axis=-1)  # (40, 3, 100, 1)
        reshaped = np.expand_dims(reshaped, axis=0)   # (1, 40, 3, 100, 1)
        
        # ğŸ”§ ê°œì„ : ì¶•ë³„ ì •ê·œí™” (ë” ì•ˆì •ì )
        for axis in range(3):
            axis_data = reshaped[0, :, axis, :, 0].flatten()
            if axis_data.std() > 1e-6:  # í‘œì¤€í¸ì°¨ê°€ 0ì´ ì•„ë‹ ë•Œë§Œ
                mean_val = axis_data.mean()
                std_val = axis_data.std()
                reshaped[0, :, axis, :, 0] = (reshaped[0, :, axis, :, 0] - mean_val) / std_val
            
        return reshaped, df, original_length
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None, None, 0

# âœ… ë°ì´í„° íŠ¹ì„± ë¶„ì„ í•¨ìˆ˜
def analyze_data_features(df):
    """ì‹¤ì‹œê°„ ë°ì´í„° íŠ¹ì„± ë¶„ì„"""
    x_data = df['sensor1_x'].values
    y_data = df['sensor1_y'].values
    z_data = df['sensor1_z'].values
    
    # 1. ìƒ˜í”Œ ìˆ˜
    sample_count = len(df)
    
    # 2. ì—ë„ˆì§€ ë¶„í¬ ê³„ì‚°
    x_energy = np.sum(x_data**2)
    y_energy = np.sum(y_data**2)
    z_energy = np.sum(z_data**2)
    total_energy = x_energy + y_energy + z_energy
    
    z_ratio = z_energy / total_energy if total_energy > 0 else 0
    
    # 3. ì£¼íŒŒìˆ˜ ë¶„ì„
    combined = np.concatenate([x_data, y_data, z_data])
    if len(combined) > 10:
        fft_result = np.fft.fft(combined)
        freqs = np.fft.fftfreq(len(combined), 1/100)
        
        # 0-50Hz ë²”ìœ„ì—ì„œ ì£¼ìš” ì£¼íŒŒìˆ˜ ì°¾ê¸°
        valid_range = (freqs >= 0) & (freqs <= 50)
        if np.any(valid_range):
            dominant_freq_idx = np.argmax(np.abs(fft_result[valid_range]))
            dominant_freq = freqs[valid_range][dominant_freq_idx]
        else:
            dominant_freq = 0
    else:
        dominant_freq = 0
    
    # 4. ì§„ë„ ë¶„ì„
    if 'sensor1_magnitude' in df.columns:
        magnitude = df['sensor1_magnitude'].values
        max_magnitude = np.max(magnitude)
        avg_magnitude = np.mean(magnitude)
    else:
        max_magnitude = 0
        avg_magnitude = 0
    
    # 5. ê·œì¹™ì„± ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
    amplitude_ranges = [np.ptp(x_data), np.ptp(y_data), np.ptp(z_data)]
    regularity_score = 1.0 / (1.0 + np.std(amplitude_ranges))  # ë²”ìœ„ê°€ ë¹„ìŠ·í• ìˆ˜ë¡ ê·œì¹™ì 
    
    return {
        'sample_count': sample_count,
        'z_energy_ratio': z_ratio,
        'dominant_freq': abs(dominant_freq),
        'max_magnitude': max_magnitude,
        'avg_magnitude': avg_magnitude,
        'regularity_score': regularity_score,
        'data_sufficient': sample_count >= 1000  # 10ì´ˆ ì´ìƒ
    }

# âœ… ê·œì¹™ ê¸°ë°˜ ë³´ì • ì‹œìŠ¤í…œ
def apply_correction_rules(predictions, features, original_length):
    """
    ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡ ë³´ì •
    """
    predicted_class = np.argmax(predictions[0])
    confidence = float(predictions[0][predicted_class])
    corrections_applied = []
    
    # ìˆ˜ì •ëœ ì˜ˆì¸¡ê°’ ë³µì‚¬
    modified_predictions = predictions[0].copy()
    
    # ê·œì¹™ 1: ë°ì´í„° ë¶€ì¡± ì‹œ ì§€ì§„ ì˜ˆì¸¡ ì–µì œ
    if original_length < 500:  # 5ì´ˆ ë¯¸ë§Œ
        if predicted_class == 0:  # ì§€ì§„ìœ¼ë¡œ ì˜ˆì¸¡í–ˆë‹¤ë©´
            modified_predictions[0] = 0.01  # ì§€ì§„ í™•ë¥  ê±°ì˜ 0ìœ¼ë¡œ
            corrections_applied.append(f"ë°ì´í„° ë¶€ì¡±({original_length}ê°œ) - ì§€ì§„ ì–µì œ")
    
    # ê·œì¹™ 2: ë§¤ìš° ë¶€ì¡±í•œ ë°ì´í„°ì—ì„œëŠ” ì§€ì§„ ì™„ì „ ë°°ì œ
    elif original_length < 200:  # 2ì´ˆ ë¯¸ë§Œ
        modified_predictions[0] = 0.0
        corrections_applied.append(f"ì‹¬ê°í•œ ë°ì´í„° ë¶€ì¡±({original_length}ê°œ) - ì§€ì§„ ì™„ì „ ë°°ì œ")
    
    # ê·œì¹™ 3: ëª¨í„° ì£¼íŒŒìˆ˜ ëŒ€ì—­ ê°ì§€ (10-30Hz)
    if 10 <= features['dominant_freq'] <= 30:
        if predicted_class == 0:  # ì§€ì§„ìœ¼ë¡œ ì˜ˆì¸¡í–ˆì§€ë§Œ ëª¨í„° ì£¼íŒŒìˆ˜
            modified_predictions[3] = max(modified_predictions[3], 0.8)  # ëª¨í„°ì§„ë™ í™•ë¥  ë†’ì„
            modified_predictions[0] = min(modified_predictions[0], 0.3)  # ì§€ì§„ í™•ë¥  ë‚®ì¶¤
            corrections_applied.append(f"ëª¨í„° ì£¼íŒŒìˆ˜({features['dominant_freq']:.1f}Hz) - ëª¨í„°ì§„ë™ ìš°ì„ ")
    
    # ê·œì¹™ 4: Zì¶• ì—ë„ˆì§€ ì§‘ì¤‘ + ë°ì´í„° ë¶€ì¡± â†’ ì‚°ì—…ì§„ë™ ê°€ëŠ¥ì„±
    if features['z_energy_ratio'] > 0.8 and original_length < 1000:
        if predicted_class == 0 and confidence < 0.9:
            modified_predictions[1] = max(modified_predictions[1], 0.7)  # ì‚°ì—…ì§„ë™ í™•ë¥  ë†’ì„
            modified_predictions[0] = min(modified_predictions[0], 0.4)  # ì§€ì§„ í™•ë¥  ë‚®ì¶¤
            corrections_applied.append(f"Zì¶• ì§‘ì¤‘({features['z_energy_ratio']:.2f}) + ë°ì´í„° ë¶€ì¡± - ì‚°ì—…ì§„ë™ ê°€ëŠ¥ì„±")
    
    # ê·œì¹™ 5: ì§€ì§„ ì‹ ë¢°ë„ ì„ê³„ê°’ (ë§¤ìš° ë†’ì€ ê¸°ì¤€)
    if predicted_class == 0 and confidence < 0.95:  # ì§€ì§„ì€ 95% ì´ìƒë§Œ ì¸ì •
        modified_predictions[0] = 0.0
        corrections_applied.append(f"ì§€ì§„ ì‹ ë¢°ë„ ë¶€ì¡±({confidence:.3f} < 0.95) - ì§€ì§„ ë°°ì œ")
    
    # ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤ ê²°ì •
    final_predicted_class = int(np.argmax(modified_predictions))
    final_confidence = float(modified_predictions[final_predicted_class])
    
    return final_predicted_class, final_confidence, corrections_applied

# âœ… ê°œì„ ëœ ì˜ˆì¸¡ ë° ì €ì¥ í•¨ìˆ˜
def predict_and_save_result_improved(filepath):
    """
    ê°œì„ ëœ 4í´ë˜ìŠ¤ ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
    """
    try:
        print(f"\nğŸ” íŒŒì¼ ë¶„ì„ ì‹œì‘: {os.path.basename(filepath)}")
        
        # 1. ê°œì„ ëœ ì „ì²˜ë¦¬
        X, df, original_length = improved_preprocessing(filepath)
        if X is None:
            return None
            
        # 2. ë°ì´í„° íŠ¹ì„± ë¶„ì„
        features = analyze_data_features(df)
        
        # 3. ì›ë³¸ ëª¨ë¸ ì˜ˆì¸¡
        start_time = time.time()
        predictions = convlstm_4class_model.predict(X, verbose=0)
        inference_time = time.time() - start_time
        
        raw_predicted_class = int(np.argmax(predictions[0]))
        raw_confidence = float(predictions[0][raw_predicted_class])
        
        # 4. ê·œì¹™ ê¸°ë°˜ ë³´ì • ì ìš©
        corrected_class, corrected_confidence, corrections = apply_correction_rules(
            predictions, features, original_length
        )
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ§  === ê°œì„ ëœ ì˜ˆì¸¡ ê²°ê³¼ ===")
        print(f"ğŸ“Š ì›ë³¸ ì˜ˆì¸¡: {CLASS_NAMES[raw_predicted_class]} ({raw_confidence:.4f})")
        
        if corrections:
            print(f"ğŸ”§ ì ìš©ëœ ë³´ì • ê·œì¹™:")
            for correction in corrections:
                print(f"   - {correction}")
        
        final_class_name = CLASS_NAMES[corrected_class]
        final_icon = CLASS_COLORS[corrected_class]
        
        print(f"{final_icon} ìµœì¢… ì˜ˆì¸¡: {final_class_name} ({corrected_confidence:.4f})")
        print(f"âš¡ ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")
        
        # 6. ìƒì„¸ íŠ¹ì„± ì •ë³´
        print(f"\nğŸ” ë°ì´í„° íŠ¹ì„± ë¶„ì„:")
        print(f"   ğŸ“ ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {original_length}ê°œ â†’ 4000ê°œë¡œ ë³€í™˜")
        print(f"   âš–ï¸ Zì¶• ì—ë„ˆì§€ ë¹„ìœ¨: {features['z_energy_ratio']:.3f}")
        print(f"   ğŸµ ì£¼ìš” ì£¼íŒŒìˆ˜: {features['dominant_freq']:.1f} Hz")
        print(f"   ğŸ“Š ìµœëŒ€ ì§„ë„: {features['max_magnitude']:.1f}")
        print(f"   ğŸ“ˆ ë°ì´í„° ì¶©ë¶„ì„±: {'âœ…' if features['data_sufficient'] else 'âŒ'}")
        
        # 7. ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥  ì¶œë ¥
        print(f"\nğŸ“Š ì „ì²´ í´ë˜ìŠ¤ë³„ í™•ë¥ :")
        for i, (class_id, class_name) in enumerate(CLASS_NAMES.items()):
            prob = predictions[0][i]
            icon = CLASS_COLORS[class_id]
            print(f"   {icon} {class_name}: {prob:.4f} ({prob*100:.2f}%)")
        
        # 8. CSVì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        if df is not None:
            df['predicted_class'] = corrected_class
            df['predicted_class_name'] = final_class_name
            df['confidence'] = corrected_confidence
            df['raw_prediction'] = raw_predicted_class
            df['raw_confidence'] = raw_confidence
            df['inference_time'] = inference_time
            df['original_sample_count'] = original_length
            df['corrections_applied'] = '; '.join(corrections) if corrections else 'None'
            
            # ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥ ë„ ì¶”ê°€
            for i, name in CLASS_NAMES.items():
                df[f'prob_{name}'] = predictions[0][i]
                
            # íŠ¹ì„± ì •ë³´ë„ ì¶”ê°€
            for key, value in features.items():
                df[f'feature_{key}'] = value
                
            # ì €ì¥
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì €ì¥ ì™„ë£Œ: {os.path.basename(filepath)}")
            
        return {
            'predicted_class': corrected_class,
            'class_name': final_class_name,
            'confidence': corrected_confidence,
            'raw_prediction': raw_predicted_class,
            'raw_confidence': raw_confidence,
            'corrections_applied': corrections,
            'features': features,
            'inference_time': inference_time
        }
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

# âœ… InfluxDB ì„¤ì •
INFLUX_TOKEN = "ZyegXlVhIdA26zFakbWjgVX863_pAtfXJPfsLGlA0wtfTxl7BHZJlMNLT5HHudXk58VzVScGnugA36w_buC4Zg=="
INFLUX_ORG = "kds"
INFLUX_BUCKET = "Lasung_3sensor of Max"
PORTS = [6060, 7001, 7053, 7060, 7070, 8010, 8080]  # ê°ì‹œ ëŒ€ìƒ í¬íŠ¸
CHECK_INTERVAL = 1  # ì´ˆ ë‹¨ìœ„ ì£¼ê¸°
COLLECTION_TIME = 60  # ğŸ”§ ê°œì„ : 40ì´ˆ â†’ 60ì´ˆë¡œ ì—°ì¥

client = InfluxDBClient(
    url="http://118.129.145.82:8086",
    token=INFLUX_TOKEN,
    org=INFLUX_ORG
)

query_api = client.query_api()

# âœ… í†µê³„ ì¶”ì ìš© ë³€ìˆ˜
detection_stats = {
    'total_events': 0,
    'class_counts': {name: 0 for name in CLASS_NAMES.values()},
    'corrections_stats': {
        'total_corrections': 0,
        'correction_types': {}
    },
    'start_time': datetime.now()
}

print("\nğŸš€ ê°œì„ ëœ 4í´ë˜ìŠ¤ ConvLSTM ì‹¤ì‹œê°„ ì§„ë™ ë¶„ë¥˜ ì‹œì‘!")
print(f"ğŸ“Š ë¶„ë¥˜ ê°€ëŠ¥í•œ í´ë˜ìŠ¤: {list(CLASS_NAMES.values())}")
print(f"ğŸ” ê°ì‹œ ëŒ€ìƒ í¬íŠ¸: {PORTS}")
print(f"â±ï¸  ì²´í¬ ì£¼ê¸°: {CHECK_INTERVAL}ì´ˆ")
print(f"ğŸ“ ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„: {COLLECTION_TIME}ì´ˆ (ê°œì„ ë¨)")
print(f"ğŸ›¡ï¸  ë³´ì • ê·œì¹™: ë°ì´í„° ë¶€ì¡±, ì£¼íŒŒìˆ˜ ê¸°ë°˜, Zì¶• ì§‘ì¤‘, ì‹ ë¢°ë„ ì„ê³„ê°’")
print("="*70)

# âœ… ê°œì„ ëœ ì‹¤ì‹œê°„ ê°ì‹œ ë£¨í”„
try:
    while True:
        now = datetime.utcnow()
        start = now - timedelta(seconds=CHECK_INTERVAL)

        for port in PORTS:
            query = f'''
            from(bucket: "{INFLUX_BUCKET}")
              |> range(start: {start.isoformat()}Z, stop: {now.isoformat()}Z)
              |> filter(fn: (r) => r._field == "intensity" and r._measurement == "{port}")
              |> sort(columns: ["_time"], desc: true)
              |> limit(n:1)
            '''

            result = query_api.query(org=INFLUX_ORG, query=query)

            for table in result:
                for record in table.records:
                    intensity = record.get_value()
                    event_time = record.get_time().astimezone()

                    if intensity >= 3.0:
                        kst_time = event_time.strftime("%Y-%m-%d %H:%M:%S")
                        print(f"\nğŸ”¥ ì§„ë„ 3 ì´ìƒ ê°ì§€: {kst_time} (í¬íŠ¸: {port}, ì§„ë„: {intensity:.2f})")
                        print(f"â³ {COLLECTION_TIME}ì´ˆ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘... (ê°œì„ ëœ ìˆ˜ì§‘ ì‹œê°„)")
                        time.sleep(COLLECTION_TIME)  # ğŸ”§ ê°œì„ : 60ì´ˆë¡œ ì—°ì¥

                        encoded_time = quote(kst_time)
                        url = f"http://192.168.68.57:1880/1min_event/{encoded_time}/{port}"

                        try:
                            res = requests.get(url)
                            print(f"ğŸŒ API í˜¸ì¶œ ê²°ê³¼: {res.status_code}")

                            if res.status_code == 200:
                                # CSV ì €ì¥
                                file_time = kst_time.replace(":", "-").replace(" ", "_")
                                filename = f"event_{port}_{file_time}_improved.csv"
                                filepath = os.path.join(OUTPUT_DIR, filename)

                                try:
                                    data = res.json()
                                    for row in data:
                                        row["timestamp"] = "'" + row["timestamp"]

                                    with open(filepath, "w", newline="", encoding="utf-8") as f:
                                        writer = csv.DictWriter(f, fieldnames=data[0].keys())
                                        writer.writeheader()
                                        writer.writerows(data)

                                    print(f"ğŸ“ ì›ë³¸ íŒŒì¼ ì €ì¥: {filename}")

                                    # ğŸ”§ ê°œì„ ëœ 4í´ë˜ìŠ¤ ì˜ˆì¸¡ ìˆ˜í–‰
                                    result = predict_and_save_result_improved(filepath)
                                    
                                    if result:
                                        # í†µê³„ ì—…ë°ì´íŠ¸
                                        detection_stats['total_events'] += 1
                                        detection_stats['class_counts'][result['class_name']] += 1
                                        
                                        # ë³´ì • í†µê³„ ì—…ë°ì´íŠ¸
                                        if result['corrections_applied']:
                                            detection_stats['corrections_stats']['total_corrections'] += 1
                                            for correction in result['corrections_applied']:
                                                correction_type = correction.split(' - ')[0]
                                                if correction_type not in detection_stats['corrections_stats']['correction_types']:
                                                    detection_stats['corrections_stats']['correction_types'][correction_type] = 0
                                                detection_stats['corrections_stats']['correction_types'][correction_type] += 1
                                        
                                        # í˜„ì¬ê¹Œì§€ í†µê³„ ì¶œë ¥
                                        print(f"\nğŸ“ˆ í˜„ì¬ê¹Œì§€ í†µê³„ (ì´ {detection_stats['total_events']}ê±´):")
                                        for class_name, count in detection_stats['class_counts'].items():
                                            if count > 0:
                                                icon = CLASS_COLORS[list(CLASS_NAMES.values()).index(class_name)]
                                                percentage = count / detection_stats['total_events'] * 100
                                                print(f"   {icon} {class_name}: {count}ê±´ ({percentage:.1f}%)")
                                        
                                        # ë³´ì • í†µê³„
                                        total_corrections = detection_stats['corrections_stats']['total_corrections']
                                        if total_corrections > 0:
                                            print(f"\nğŸ”§ ë³´ì • í†µê³„:")
                                            print(f"   ì „ì²´ ë³´ì • ì ìš©: {total_corrections}/{detection_stats['total_events']}ê±´ ({total_corrections/detection_stats['total_events']*100:.1f}%)")
                                            for correction_type, count in detection_stats['corrections_stats']['correction_types'].items():
                                                print(f"   - {correction_type}: {count}ê±´")
                                        
                                        runtime = datetime.now() - detection_stats['start_time']
                                        print(f"ğŸ•’ ì‹¤í–‰ ì‹œê°„: {runtime}")

                                except Exception as e:
                                    print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

                        except Exception as e:
                            print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")

        time.sleep(CHECK_INTERVAL)

except KeyboardInterrupt:
    print("\n\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    print("ğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   ì´ ì´ë²¤íŠ¸: {detection_stats['total_events']}ê±´")
    for class_name, count in detection_stats['class_counts'].items():
        if count > 0:
            icon = CLASS_COLORS[list(CLASS_NAMES.values()).index(class_name)]
            percentage = count / detection_stats['total_events'] * 100 if detection_stats['total_events'] > 0 else 0
            print(f"   {icon} {class_name}: {count}ê±´ ({percentage:.1f}%)")
    
    total_corrections = detection_stats['corrections_stats']['total_corrections']
    if total_corrections > 0:
        print(f"\nğŸ”§ ìµœì¢… ë³´ì • í†µê³„:")
        print(f"   ì „ì²´ ë³´ì • ì ìš©: {total_corrections}/{detection_stats['total_events']}ê±´")
        for correction_type, count in detection_stats['corrections_stats']['correction_types'].items():
            print(f"   - {correction_type}: {count}ê±´")
    
    print("ğŸ‘‹ ê°œì„ ëœ ì‹œìŠ¤í…œ ì¢…ë£Œ")

except Exception as e:
    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()