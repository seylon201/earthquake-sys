import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class SimplifiedEarthquakeModel:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_names = None
        self.results = {}
        
    def load_preprocessed_data(self):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        """
        print("ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ")
        print("="*50)
        
        try:
            # ìµœì‹  íŒŒì¼ë“¤ ì°¾ê¸°
            import glob
            
            train_files = glob.glob("earthquake_train_*.csv")
            val_files = glob.glob("earthquake_val_*.csv") 
            test_files = glob.glob("earthquake_test_*.csv")
            
            if not all([train_files, val_files, test_files]):
                raise FileNotFoundError("ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ê°€ì¥ ìµœì‹  íŒŒì¼ë“¤ ì„ íƒ
            train_file = max(train_files, key=lambda x: x.split('_')[-1])
            val_file = max(val_files, key=lambda x: x.split('_')[-1])
            test_file = max(test_files, key=lambda x: x.split('_')[-1])
            
            print(f"ğŸ“ í›ˆë ¨ ë°ì´í„°: {train_file}")
            print(f"ğŸ“ ê²€ì¦ ë°ì´í„°: {val_file}")
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_file}")
            
            # ë°ì´í„° ë¡œë“œ
            self.train_data = pd.read_csv(train_file)
            self.val_data = pd.read_csv(val_file)
            self.test_data = pd.read_csv(test_file)
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ")
            print(f"   í›ˆë ¨: {len(self.train_data):,}ê°œ")
            print(f"   ê²€ì¦: {len(self.val_data):,}ê°œ")
            print(f"   í…ŒìŠ¤íŠ¸: {len(self.test_data):,}ê°œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def prepare_features(self):
        """
        ëª¨ë¸ìš© íŠ¹ì„± ì¤€ë¹„
        """
        print("\nğŸ”§ ëª¨ë¸ìš© íŠ¹ì„± ì¤€ë¹„")
        print("="*50)
        
        # í•µì‹¬ íŠ¹ì„± ì„ íƒ
        core_features = [
            'mag', 'latitude', 'longitude', 'depth',  # ê¸°ë³¸ ì§€ì§„ íŠ¹ì„±
            'year', 'month', 'day', 'hour', 'season',  # ì‹œê°„ íŠ¹ì„±
            'region_code', 'distance_to_sf', 'distance_to_la',  # ì§€ì—­ íŠ¹ì„±
            'energy_relative', 'network_activity_level', 'lunar_phase',  # ê³ ê¸‰ íŠ¹ì„±
            'state_encoded', 'net_encoded', 'magType_encoded', 'faultType_encoded', 'type_encoded'  # ì¸ì½”ë”© íŠ¹ì„±
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±ë§Œ ì„ íƒ
        available_features = [col for col in core_features if col in self.train_data.columns]
        self.feature_names = available_features
        
        print(f"ğŸ“Š ì„ íƒëœ íŠ¹ì„± ({len(available_features)}ê°œ):")
        for i, feature in enumerate(available_features):
            print(f"   {i+1:2d}. {feature}")
        
        # íŠ¹ì„± ë°ì´í„° ì¶”ì¶œ
        X_train = self.train_data[available_features]
        X_val = self.val_data[available_features]
        X_test = self.test_data[available_features]
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        X_train = X_train.fillna(X_train.median())
        X_val = X_val.fillna(X_train.median())
        X_test = X_test.fillna(X_train.median())
        
        print(f"âœ… íŠ¹ì„± ì¤€ë¹„ ì™„ë£Œ")
        print(f"   í›ˆë ¨ íŠ¹ì„± í˜•íƒœ: {X_train.shape}")
        print(f"   ê²€ì¦ íŠ¹ì„± í˜•íƒœ: {X_val.shape}")
        print(f"   í…ŒìŠ¤íŠ¸ íŠ¹ì„± í˜•íƒœ: {X_test.shape}")
        
        return X_train, X_val, X_test
    
    def prepare_labels(self):
        """
        ë¼ë²¨ ì¤€ë¹„
        """
        print("\nğŸ·ï¸ ë¼ë²¨ ì¤€ë¹„")
        print("="*50)
        
        # ì§„ë„ ê¸°ë°˜ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¼ë²¨ ìƒì„±
        def create_magnitude_classes(data):
            mag_values = data['mag'].values
            
            # ì§„ë„ ê¸°ë°˜ 6í´ë˜ìŠ¤ ë¶„ë¥˜
            labels = np.zeros(len(mag_values))
            labels[(mag_values >= 3.0) & (mag_values < 3.5)] = 0  # ì†Œê·œëª¨
            labels[(mag_values >= 3.5) & (mag_values < 4.0)] = 1  # ì†Œ-ì¤‘ê°„
            labels[(mag_values >= 4.0) & (mag_values < 4.5)] = 2  # ì¤‘ê°„
            labels[(mag_values >= 4.5) & (mag_values < 5.0)] = 3  # ì¤‘-ëŒ€
            labels[(mag_values >= 5.0) & (mag_values < 5.5)] = 4  # ëŒ€ê·œëª¨
            labels[(mag_values >= 5.5)] = 5  # ì£¼ìš” ì§€ì§„
            
            return labels.astype(int)
        
        y_train = create_magnitude_classes(self.train_data)
        y_val = create_magnitude_classes(self.val_data)
        y_test = create_magnitude_classes(self.test_data)
        
        self.class_names = ['3.0-3.4', '3.5-3.9', '4.0-4.4', '4.5-4.9', '5.0-5.4', '5.5+']
        
        print(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
        for i, name in enumerate(self.class_names):
            count = np.sum(y_train == i)
            print(f"   í´ë˜ìŠ¤ {i} ({name}): {count}ê°œ")
        
        return y_train, y_val, y_test
    
    def train_multiple_models(self, X_train, y_train, X_val, y_val):
        """
        ì—¬ëŸ¬ ëª¨ë¸ í›ˆë ¨ ë° ë¹„êµ
        """
        print("\nğŸƒâ€â™‚ï¸ ì—¬ëŸ¬ ëª¨ë¸ í›ˆë ¨ ë° ë¹„êµ")
        print("="*50)
        
        # ë°ì´í„° í•©ì¹˜ê¸° (í›ˆë ¨ + ê²€ì¦)
        X_combined = pd.concat([X_train, X_val])
        y_combined = np.concatenate([y_train, y_val])
        
        # ì •ê·œí™”
        scaler = StandardScaler()
        X_combined_scaled = scaler.fit_transform(X_combined)
        self.scaler = scaler
        
        # ëª¨ë¸ë“¤ ì •ì˜
        models_to_train = {
            'Random Forest': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=150,
                max_depth=8,
                learning_rate=0.1,
                random_state=42
            ),
            'Logistic Regression': LogisticRegression(
                max_iter=1000,
                random_state=42,
                multi_class='ovr'
            ),
            'SVM': SVC(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                random_state=42,
                probability=True
            )
        }
        
        # ê° ëª¨ë¸ í›ˆë ¨
        trained_models = {}
        for name, model in models_to_train.items():
            print(f"ğŸ”„ {name} í›ˆë ¨ ì¤‘...")
            
            try:
                model.fit(X_combined_scaled, y_combined)
                trained_models[name] = model
                print(f"   âœ… {name} í›ˆë ¨ ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ {name} í›ˆë ¨ ì‹¤íŒ¨: {e}")
        
        self.models = trained_models
        print(f"âœ… ì´ {len(trained_models)}ê°œ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        
        return trained_models
    
    def evaluate_models(self, X_test, y_test):
        """
        ëª¨ë¸ë“¤ í‰ê°€
        """
        print("\nğŸ“Š ëª¨ë¸ í‰ê°€")
        print("="*50)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ê·œí™”
        X_test_scaled = self.scaler.transform(X_test)
        
        results = {}
        
        for name, model in self.models.items():
            print(f"\nğŸ” {name} í‰ê°€:")
            
            try:
                # ì˜ˆì¸¡
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None
                
                # ì •í™•ë„
                accuracy = accuracy_score(y_test, y_pred)
                print(f"   ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")
                
                # ë¶„ë¥˜ ë¦¬í¬íŠ¸
                report = classification_report(y_test, y_pred, target_names=self.class_names, output_dict=True)
                
                # í˜¼ë™ í–‰ë ¬
                cm = confusion_matrix(y_test, y_pred)
                
                results[name] = {
                    'accuracy': accuracy,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba,
                    'classification_report': report,
                    'confusion_matrix': cm
                }
                
                # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìš”ì•½
                print(f"   í´ë˜ìŠ¤ë³„ F1 ì ìˆ˜:")
                for i, class_name in enumerate(self.class_names):
                    if class_name in report:
                        f1 = report[class_name]['f1-score']
                        print(f"     {class_name}: {f1:.3f}")
                
            except Exception as e:
                print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        
        self.results = results
        return results
    
    def analyze_regional_performance(self, X_test, y_test):
        """
        ì§€ì—­ë³„ ì„±ëŠ¥ ë¶„ì„
        """
        print("\nğŸŒ ì§€ì—­ë³„ ì„±ëŠ¥ ë¶„ì„")
        print("="*50)
        
        if 'region' not in self.test_data.columns:
            print("âš ï¸ ì§€ì—­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        regions = self.test_data['region'].values
        unique_regions = np.unique(regions)
        
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"ğŸ“Š ì§€ì—­ë³„ ì„±ëŠ¥ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê¸°ì¤€):")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model_name = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        best_model = self.models[best_model_name]
        
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (ì •í™•ë„: {self.results[best_model_name]['accuracy']:.4f})")
        
        regional_performance = {}
        
        for region in unique_regions:
            region_mask = regions == region
            region_count = np.sum(region_mask)
            
            if region_count > 0:
                X_region = X_test_scaled[region_mask]
                y_region = y_test[region_mask]
                
                y_pred_region = best_model.predict(X_region)
                accuracy_region = accuracy_score(y_region, y_pred_region)
                
                regional_performance[region] = {
                    'accuracy': accuracy_region,
                    'sample_count': region_count
                }
                
                print(f"   {region}: {accuracy_region:.4f} ({region_count}ê°œ ìƒ˜í”Œ)")
        
        return regional_performance
    
    def analyze_feature_importance(self):
        """
        íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        """
        print("\nğŸ“ˆ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
        print("="*50)
        
        # Random Forestì˜ íŠ¹ì„± ì¤‘ìš”ë„ ì‚¬ìš©
        if 'Random Forest' in self.models:
            rf_model = self.models['Random Forest']
            feature_importance = rf_model.feature_importances_
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ì •ë ¬
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': feature_importance
            }).sort_values('importance', ascending=False)
            
            print(f"ğŸ” ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
            for i, row in importance_df.head(10).iterrows():
                print(f"   {row['feature']}: {row['importance']:.4f}")
            
            return importance_df
        else:
            print("âš ï¸ Random Forest ëª¨ë¸ì´ ì—†ì–´ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def visualize_results(self):
        """
        ê²°ê³¼ ì‹œê°í™”
        """
        print("\nğŸ“Š ê²°ê³¼ ì‹œê°í™”")
        print("="*50)
        
        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        model_names = list(self.results.keys())
        accuracies = [self.results[name]['accuracy'] for name in model_names]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        ax1 = axes[0, 0]
        bars = ax1.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
        ax1.set_title('ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ', fontsize=14, fontweight='bold')
        ax1.set_xlabel('ëª¨ë¸')
        ax1.set_ylabel('ì •í™•ë„')
        ax1.set_ylim(0, 1)
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom')
        
        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
        
        # 2. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬
        best_model_name = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        best_cm = self.results[best_model_name]['confusion_matrix']
        
        ax2 = axes[0, 1]
        sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names, yticklabels=self.class_names, ax=ax2)
        ax2.set_title(f'{best_model_name} í˜¼ë™ í–‰ë ¬', fontsize=14, fontweight='bold')
        ax2.set_xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤')
        ax2.set_ylabel('ì‹¤ì œ í´ë˜ìŠ¤')
        
        # 3. í´ë˜ìŠ¤ë³„ F1 ì ìˆ˜ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)
        ax3 = axes[1, 0]
        best_report = self.results[best_model_name]['classification_report']
        f1_scores = [best_report[class_name]['f1-score'] for class_name in self.class_names if class_name in best_report]
        
        bars = ax3.bar(range(len(self.class_names)), f1_scores, color='lightgreen')
        ax3.set_title(f'{best_model_name} í´ë˜ìŠ¤ë³„ F1 ì ìˆ˜', fontsize=14, fontweight='bold')
        ax3.set_xlabel('ì§„ë„ í´ë˜ìŠ¤')
        ax3.set_ylabel('F1 ì ìˆ˜')
        ax3.set_xticks(range(len(self.class_names)))
        ax3.set_xticklabels(self.class_names, rotation=45)
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom')
        
        # 4. íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)
        ax4 = axes[1, 1]
        if 'Random Forest' in self.models:
            rf_model = self.models['Random Forest']
            feature_importance = rf_model.feature_importances_
            
            # ìƒìœ„ 10ê°œ íŠ¹ì„±
            top_indices = np.argsort(feature_importance)[-10:]
            top_features = [self.feature_names[i] for i in top_indices]
            top_importance = feature_importance[top_indices]
            
            bars = ax4.barh(range(len(top_features)), top_importance, color='orange')
            ax4.set_title('ìƒìœ„ 10ê°œ íŠ¹ì„± ì¤‘ìš”ë„', fontsize=14, fontweight='bold')
            ax4.set_xlabel('ì¤‘ìš”ë„')
            ax4.set_yticks(range(len(top_features)))
            ax4.set_yticklabels(top_features)
        
        plt.tight_layout()
        
        # ì €ì¥
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"earthquake_model_results_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ê²°ê³¼ ì‹œê°í™” ì €ì¥: {filename}")
        
        plt.show()
        return filename
    
    def save_results(self):
        """
        ê²°ê³¼ ì €ì¥
        """
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥")
        print("="*50)
        
        from datetime import datetime
        import json
        import joblib
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        best_model_name = max(self.results.keys(), key=lambda k: self.results[k]['accuracy'])
        best_model = self.models[best_model_name]
        
        model_filename = f"best_earthquake_model_{timestamp}.pkl"
        joblib.dump({
            'model': best_model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'class_names': self.class_names,
            'model_name': best_model_name
        }, model_filename)
        
        print(f"ğŸ§  ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {model_filename}")
        print(f"   ëª¨ë¸: {best_model_name}")
        print(f"   ì •í™•ë„: {self.results[best_model_name]['accuracy']:.4f}")
        
        # 2. ê²°ê³¼ ìš”ì•½ ì €ì¥
        results_summary = {
            'timestamp': timestamp,
            'model_comparison': {
                name: {
                    'accuracy': float(results['accuracy']),
                    'class_performance': {
                        class_name: {
                            'precision': float(results['classification_report'][class_name]['precision']),
                            'recall': float(results['classification_report'][class_name]['recall']),
                            'f1_score': float(results['classification_report'][class_name]['f1-score'])
                        }
                        for class_name in self.class_names 
                        if class_name in results['classification_report']
                    }
                }
                for name, results in self.results.items()
            },
            'best_model': {
                'name': best_model_name,
                'accuracy': float(self.results[best_model_name]['accuracy']),
                'feature_count': len(self.feature_names)
            }
        }
        
        results_filename = f"earthquake_model_results_{timestamp}.json"
        with open(results_filename, 'w', encoding='utf-8') as f:
            json.dump(results_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½ ì €ì¥: {results_filename}")
        
        return model_filename, results_filename

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    print("ğŸš€ í˜¸í™˜ì„± ê°œì„ ëœ ì§€ì§„ ê°ì§€ ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    earthquake_model = SimplifiedEarthquakeModel()
    
    # 1. ë°ì´í„° ë¡œë“œ
    if not earthquake_model.load_preprocessed_data():
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. íŠ¹ì„± ì¤€ë¹„
    X_train, X_val, X_test = earthquake_model.prepare_features()
    
    # 3. ë¼ë²¨ ì¤€ë¹„
    y_train, y_val, y_test = earthquake_model.prepare_labels()
    
    # 4. ì—¬ëŸ¬ ëª¨ë¸ í›ˆë ¨
    models = earthquake_model.train_multiple_models(X_train, y_train, X_val, y_val)
    
    if not models:
        print("âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 5. ëª¨ë¸ í‰ê°€
    results = earthquake_model.evaluate_models(X_test, y_test)
    
    # 6. ì§€ì—­ë³„ ì„±ëŠ¥ ë¶„ì„
    regional_perf = earthquake_model.analyze_regional_performance(X_test, y_test)
    
    # 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    feature_importance = earthquake_model.analyze_feature_importance()
    
    # 8. ê²°ê³¼ ì‹œê°í™”
    plot_filename = earthquake_model.visualize_results()
    
    # 9. ê²°ê³¼ ì €ì¥
    model_file, results_file = earthquake_model.save_results()
    
    # ìµœì¢… ìš”ì•½
    print(f"\nğŸ‰ ì§€ì§„ ê°ì§€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    best_model_name = max(results.keys(), key=lambda k: results[k]['accuracy'])
    best_accuracy = results[best_model_name]['accuracy']
    
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"ğŸ“Š ìµœê³  ì •í™•ë„: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"   - ëª¨ë¸: {model_file}")
    print(f"   - ê²°ê³¼: {results_file}")
    print(f"   - ì‹œê°í™”: {plot_filename}")
    
    print(f"\nğŸ’¡ ì„±ëŠ¥ ë¶„ì„:")
    if best_accuracy > 0.95:
        print(f"   ğŸ¯ ìš°ìˆ˜í•œ ì„±ëŠ¥! ê¸°ì¡´ 98% ëŒ€ë¹„ ê°œì„  ë‹¬ì„±")
    elif best_accuracy > 0.90:
        print(f"   âœ… ì–‘í˜¸í•œ ì„±ëŠ¥ (ëª©í‘œ 90% ì´ìƒ ë‹¬ì„±)")
    else:
        print(f"   âš ï¸ ì¶”ê°€ íŠœë‹ í•„ìš”")
    
    print(f"\nğŸŒ ë‹¤ì§€ì—­ ëŒ€ì‘ ì„±ê³¼:")
    print(f"   âœ… ë™ì•„ì‹œì•„ + ë¶ë¯¸ + íƒœí‰ì–‘ í†µí•© í•™ìŠµ")
    print(f"   âœ… ì§„ë„ 3.0-6.0 ì „ ë²”ìœ„ ì»¤ë²„")
    print(f"   âœ… 20ê°œ ê³ ê¸‰ íŠ¹ì„±ìœ¼ë¡œ ì •ë°€ ë¶„ë¥˜")

if __name__ == "__main__":
    main()