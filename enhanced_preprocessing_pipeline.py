import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

class EnhancedEarthquakePreprocessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.minmax_scaler = MinMaxScaler()
        self.label_encoders = {}
        self.feature_importance = {}
        
    def load_integrated_dataset(self, filename=None):
        """
        í†µí•© ë°ì´í„°ì…‹ ë¡œë“œ
        """
        if filename is None:
            # ê°€ì¥ ìµœì‹  í†µí•© ë°ì´í„°ì…‹ íŒŒì¼ ì°¾ê¸°
            import glob
            files = glob.glob("integrated_earthquake_dataset_*.csv")
            if not files:
                raise FileNotFoundError("í†µí•© ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            filename = max(files, key=lambda x: x.split('_')[-1])
        
        print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ: {filename}")
        
        try:
            self.data = pd.read_csv(filename)
            print(f"âœ… ë¡œë“œ ì„±ê³µ: {len(self.data)}ê°œ ë ˆì½”ë“œ, {len(self.data.columns)}ê°œ ì»¬ëŸ¼")
            return self.data
        except Exception as e:
            print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_feature_distribution(self):
        """
        íŠ¹ì„± ë¶„í¬ ë¶„ì„ ë° ì´ìƒê°’ ê°ì§€
        """
        print("\nğŸ” íŠ¹ì„± ë¶„í¬ ë¶„ì„")
        print("="*50)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¶„ì„
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        
        print(f"ğŸ“Š ì»¬ëŸ¼ íƒ€ì… ë¶„ì„:")
        print(f"   ìˆ˜ì¹˜í˜•: {len(numeric_cols)}ê°œ")
        print(f"   ë²”ì£¼í˜•: {len(categorical_cols)}ê°œ")
        
        # ì£¼ìš” ìˆ˜ì¹˜í˜• íŠ¹ì„± ë¶„ì„
        key_features = ['mag', 'latitude', 'longitude', 'depth', 'year']
        available_features = [col for col in key_features if col in numeric_cols]
        
        print(f"\nğŸ“ˆ ì£¼ìš” ìˆ˜ì¹˜í˜• íŠ¹ì„± ë¶„ì„:")
        for feature in available_features:
            if feature in self.data.columns:
                stats = self.data[feature].describe()
                print(f"   {feature}:")
                print(f"     ë²”ìœ„: {stats['min']:.2f} ~ {stats['max']:.2f}")
                print(f"     í‰ê· : {stats['mean']:.2f}, í‘œì¤€í¸ì°¨: {stats['std']:.2f}")
                
                # ì´ìƒê°’ ê°ì§€ (IQR ë°©ë²•)
                Q1 = stats['25%']
                Q3 = stats['75%']
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = self.data[(self.data[feature] < lower_bound) | 
                                   (self.data[feature] > upper_bound)]
                if len(outliers) > 0:
                    print(f"     âš ï¸ ì´ìƒê°’: {len(outliers)}ê°œ ({len(outliers)/len(self.data)*100:.1f}%)")
                else:
                    print(f"     âœ… ì´ìƒê°’ ì—†ìŒ")
        
        # ë²”ì£¼í˜• íŠ¹ì„± ë¶„ì„
        print(f"\nğŸ“‹ ì£¼ìš” ë²”ì£¼í˜• íŠ¹ì„± ë¶„ì„:")
        key_categorical = ['region', 'net', 'magType', 'event_type']
        for feature in key_categorical:
            if feature in self.data.columns:
                value_counts = self.data[feature].value_counts()
                print(f"   {feature}: {len(value_counts)}ê°œ ì¹´í…Œê³ ë¦¬")
                print(f"     ìƒìœ„ 3ê°œ: {dict(value_counts.head(3))}")
        
        return numeric_cols, categorical_cols
    
    def create_advanced_features(self):
        """
        ê³ ê¸‰ íŠ¹ì„± ìƒì„±
        """
        print("\nğŸ”§ ê³ ê¸‰ íŠ¹ì„± ìƒì„±")
        print("="*50)
        
        # 1. ì§€ë¦¬ì  íŠ¹ì„±
        if 'latitude' in self.data.columns and 'longitude' in self.data.columns:
            print("ğŸŒ ì§€ë¦¬ì  íŠ¹ì„± ìƒì„±...")
            
            # ê±°ë¦¬ ê¸°ë°˜ íŠ¹ì„± (ì£¼ìš” ë„ì‹œ/ë‹¨ì¸µì„ ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬)
            # ìƒŒí”„ë€ì‹œìŠ¤ì½” (37.7749, -122.4194)
            sf_lat, sf_lon = 37.7749, -122.4194
            self.data['distance_to_sf'] = np.sqrt(
                (self.data['latitude'] - sf_lat)**2 + 
                (self.data['longitude'] - sf_lon)**2
            )
            
            # ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤ (34.0522, -118.2437)
            la_lat, la_lon = 34.0522, -118.2437
            self.data['distance_to_la'] = np.sqrt(
                (self.data['latitude'] - la_lat)**2 + 
                (self.data['longitude'] - la_lon)**2
            )
            
            # ìœ„ë„ëŒ€ êµ¬ë¶„
            self.data['latitude_zone'] = pd.cut(self.data['latitude'], 
                                              bins=[-90, 0, 30, 60, 90], 
                                              labels=['Southern', 'Tropical', 'Temperate', 'Northern'])
            
            print("   âœ… ì§€ë¦¬ì  íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        
        # 2. ì‹œê°„ì  íŠ¹ì„±
        if 'datetime' in self.data.columns:
            print("â° ì‹œê°„ì  íŠ¹ì„± ìƒì„±...")
            
            try:
                self.data['datetime'] = pd.to_datetime(self.data['datetime'])
                
                # ê³„ì ˆ ì •ë³´
                self.data['season'] = self.data['datetime'].dt.month % 12 // 3 + 1
                season_map = {1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'}
                self.data['season_name'] = self.data['season'].map(season_map)
                
                # í•˜ë£¨ ì¤‘ ì‹œê°„ëŒ€
                self.data['time_of_day'] = pd.cut(self.data['hour'], 
                                                bins=[0, 6, 12, 18, 24], 
                                                labels=['Night', 'Morning', 'Afternoon', 'Evening'],
                                                include_lowest=True)
                
                # ì§€ì§„ í™œë™ ì£¼ê¸° (lunar cycle ê·¼ì‚¬)
                days_since_epoch = (self.data['datetime'] - pd.Timestamp('2000-01-01')).dt.days
                self.data['lunar_phase'] = (days_since_epoch % 29.5) / 29.5
                
                print("   âœ… ì‹œê°„ì  íŠ¹ì„± ìƒì„± ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ ì‹œê°„ì  íŠ¹ì„± ìƒì„± ì‹¤íŒ¨: {e}")
        
        # 3. ì§„ë„ ê´€ë ¨ íŠ¹ì„±
        if 'mag' in self.data.columns:
            print("âš¡ ì§„ë„ ê´€ë ¨ íŠ¹ì„± ìƒì„±...")
            
            # ì—ë„ˆì§€ ì¶”ì • (ë¦¬íˆí„° ìŠ¤ì¼€ì¼ ê¸°ë°˜)
            self.data['energy_log'] = 11.8 + 1.5 * self.data['mag']  # log10(ì—ë„ˆì§€)
            self.data['energy_relative'] = 10 ** (1.5 * (self.data['mag'] - 3.0))  # ì§„ë„ 3 ê¸°ì¤€ ìƒëŒ€ ì—ë„ˆì§€
            
            # ì§„ë„ ì¹´í…Œê³ ë¦¬ (ë” ì„¸ë¶„í™”)
            mag_bins = [0, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 10.0]
            mag_labels = ['micro', 'minor', 'light', 'moderate', 'strong', 'major', 'great', 'extreme']
            self.data['magnitude_category'] = pd.cut(self.data['mag'], bins=mag_bins, labels=mag_labels)
            
            # ì§„ë„ ì •ê·œí™” (ì—¬ëŸ¬ ë°©ë²•)
            self.data['mag_normalized'] = (self.data['mag'] - self.data['mag'].min()) / (self.data['mag'].max() - self.data['mag'].min())
            self.data['mag_standardized'] = (self.data['mag'] - self.data['mag'].mean()) / self.data['mag'].std()
            
            print("   âœ… ì§„ë„ ê´€ë ¨ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        
        # 4. ë„¤íŠ¸ì›Œí¬/ì§€ì—­ íŠ¹ì„±
        if 'net' in self.data.columns:
            print("ğŸŒ ë„¤íŠ¸ì›Œí¬ íŠ¹ì„± ìƒì„±...")
            
            # ë„¤íŠ¸ì›Œí¬ í™œë™ ìˆ˜ì¤€
            network_counts = self.data['net'].value_counts()
            self.data['network_activity_level'] = self.data['net'].map(network_counts)
            
            # ì£¼ìš” ë„¤íŠ¸ì›Œí¬ ì—¬ë¶€
            major_networks = ['ci', 'CI', 'NC', 'nc', 'us', 'ak']
            self.data['is_major_network'] = self.data['net'].isin(major_networks)
            
            print("   âœ… ë„¤íŠ¸ì›Œí¬ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        
        print(f"ğŸ‰ íŠ¹ì„± ìƒì„± ì™„ë£Œ! ì´ {len(self.data.columns)}ê°œ ì»¬ëŸ¼")
        return self.data
    
    def handle_missing_values(self):
        """
        ê²°ì¸¡ê°’ ì²˜ë¦¬
        """
        print("\nğŸ”§ ê²°ì¸¡ê°’ ì²˜ë¦¬")
        print("="*50)
        
        missing_before = self.data.isnull().sum().sum()
        print(f"ì²˜ë¦¬ ì „ ê²°ì¸¡ê°’: {missing_before}ê°œ")
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if self.data[col].isnull().sum() > 0:
                median_val = self.data[col].median()
                self.data[col].fillna(median_val, inplace=True)
                print(f"   {col}: ì¤‘ê°„ê°’ {median_val:.2f}ë¡œ ëŒ€ì²´")
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if self.data[col].isnull().sum() > 0:
                mode_val = self.data[col].mode().iloc[0] if len(self.data[col].mode()) > 0 else 'Unknown'
                self.data[col].fillna(mode_val, inplace=True)
                print(f"   {col}: ìµœë¹ˆê°’ '{mode_val}'ë¡œ ëŒ€ì²´")
        
        missing_after = self.data.isnull().sum().sum()
        print(f"âœ… ì²˜ë¦¬ í›„ ê²°ì¸¡ê°’: {missing_after}ê°œ")
        
        return self.data
    
    def encode_categorical_features(self):
        """
        ë²”ì£¼í˜• íŠ¹ì„± ì¸ì½”ë”©
        """
        print("\nğŸ”¢ ë²”ì£¼í˜• íŠ¹ì„± ì¸ì½”ë”©")
        print("="*50)
        
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        
        # ê¸°ì¡´ í”„ë¡œì íŠ¸ í˜¸í™˜ì„ ìœ„í•œ íŠ¹ë³„ ì²˜ë¦¬
        priority_cols = ['region', 'net', 'magType', 'magnitude_category']
        
        for col in categorical_cols:
            if col in priority_cols or self.data[col].nunique() <= 20:  # ì¹´í…Œê³ ë¦¬ê°€ 20ê°œ ì´í•˜ì¸ ê²½ìš°ë§Œ
                le = LabelEncoder()
                self.data[f'{col}_encoded'] = le.fit_transform(self.data[col].astype(str))
                self.label_encoders[col] = le
                print(f"   {col}: {self.data[col].nunique()}ê°œ ì¹´í…Œê³ ë¦¬ â†’ ìˆ˜ì¹˜í˜• ë³€í™˜")
        
        print(f"âœ… {len(self.label_encoders)}ê°œ ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”© ì™„ë£Œ")
        return self.data
    
    def create_convlstm_features(self):
        """
        ConvLSTM ëª¨ë¸ìš© íŠ¹ì„± ìƒì„±
        """
        print("\nğŸ§  ConvLSTM ëª¨ë¸ìš© íŠ¹ì„± ìƒì„±")
        print("="*50)
        
        # í•µì‹¬ íŠ¹ì„± ì„ íƒ
        core_features = []
        
        # 1. í•„ìˆ˜ ì§€ì§„ íŠ¹ì„±
        essential_features = ['mag', 'latitude', 'longitude', 'depth']
        for feature in essential_features:
            if feature in self.data.columns:
                core_features.append(feature)
        
        # 2. ì‹œê°„ íŠ¹ì„±
        time_features = ['year', 'month', 'day', 'hour', 'season']
        for feature in time_features:
            if feature in self.data.columns:
                core_features.append(feature)
        
        # 3. ì§€ì—­ íŠ¹ì„±
        region_features = ['region_code', 'distance_to_sf', 'distance_to_la']
        for feature in region_features:
            if feature in self.data.columns:
                core_features.append(feature)
        
        # 4. ê³ ê¸‰ íŠ¹ì„±
        advanced_features = ['energy_relative', 'network_activity_level', 'lunar_phase']
        for feature in advanced_features:
            if feature in self.data.columns:
                core_features.append(feature)
        
        # 5. ë²”ì£¼í˜• ì¸ì½”ë”©ëœ íŠ¹ì„±
        encoded_features = [col for col in self.data.columns if col.endswith('_encoded')]
        core_features.extend(encoded_features[:5])  # ìƒìœ„ 5ê°œë§Œ
        
        # íŠ¹ì„± í–‰ë ¬ ìƒì„±
        self.feature_matrix = self.data[core_features].copy()
        
        print(f"ğŸ“Š ì„ íƒëœ íŠ¹ì„± ({len(core_features)}ê°œ):")
        for i, feature in enumerate(core_features):
            print(f"   {i+1:2d}. {feature}")
        
        # ì •ê·œí™”
        feature_scaled = self.scaler.fit_transform(self.feature_matrix)
        self.feature_matrix_scaled = pd.DataFrame(feature_scaled, columns=core_features)
        
        print(f"âœ… ConvLSTM íŠ¹ì„± í–‰ë ¬ ìƒì„± ì™„ë£Œ: {self.feature_matrix_scaled.shape}")
        
        return self.feature_matrix_scaled, core_features
    
    def prepare_for_existing_project(self):
        """
        ê¸°ì¡´ í”„ë¡œì íŠ¸ í˜•íƒœë¡œ ë°ì´í„° ì¤€ë¹„
        """
        print("\nğŸ”„ ê¸°ì¡´ í”„ë¡œì íŠ¸ í˜¸í™˜ í˜•íƒœë¡œ ë³€í™˜")
        print("="*50)
        
        # ê¸°ì¡´ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì»¬ëŸ¼ í˜•íƒœë¡œ ë³€í™˜
        project_data = self.data.copy()
        
        # í´ë˜ìŠ¤ ë¼ë²¨ (ê¸°ì¡´ í”„ë¡œì íŠ¸: ì§€ì§„=0, ë¶ˆê·œì¹™ìƒí™œì§„ë™=1, ê·œì¹™ì ì‚°ì—…ì§„ë™=2)
        project_data['class_label'] = 0  # ëª¨ë“  ë°ì´í„°ê°€ ì§€ì§„
        
        # ê¸°ì¡´ í”„ë¡œì íŠ¸ì˜ ì£¼ìš” íŠ¹ì„±ë“¤
        required_columns = {
            'magnitude': 'mag',
            'event_time': 'datetime', 
            'event_latitude': 'latitude',
            'event_longitude': 'longitude',
            'event_depth': 'depth',
            'event_region': 'region_code',
            'data_source': 'net'
        }
        
        for new_col, old_col in required_columns.items():
            if old_col in project_data.columns:
                project_data[new_col] = project_data[old_col]
        
        # 40ì´ˆ ìŠ¬ë¼ì´ì‹±ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° (ì‹œë®¬ë ˆì´ì…˜)
        project_data['trigger_timestamp'] = project_data['datetime']
        project_data['sampling_rate'] = 100  # 100Hz
        project_data['pre_trigger_seconds'] = 15
        project_data['post_trigger_seconds'] = 25
        project_data['total_samples'] = 4000  # 40ì´ˆ * 100Hz
        
        # 3ì¶• ê°€ì†ë„ ì‹œë®¬ë ˆì´ì…˜ ë©”íƒ€ë°ì´í„°
        project_data['has_xyz_data'] = True
        project_data['preprocessing_applied'] = 'z_score_normalized'
        project_data['tensor_shape'] = '(40, 3, 100, 1)'
        
        print(f"âœ… ê¸°ì¡´ í”„ë¡œì íŠ¸ í˜¸í™˜ ë°ì´í„° ìƒì„±")
        print(f"   ë ˆì½”ë“œ ìˆ˜: {len(project_data)}")
        print(f"   ìƒˆë¡œìš´ ì»¬ëŸ¼: {list(required_columns.keys())}")
        
        return project_data
    
    def create_train_test_split(self, test_size=0.2, validation_size=0.2):
        """
        í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        """
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  (í›ˆë ¨:{1-test_size-validation_size:.1f}, ê²€ì¦:{validation_size:.1f}, í…ŒìŠ¤íŠ¸:{test_size:.1f})")
        print("="*50)
        
        # ì§€ì—­ë³„ stratified split
        if 'region_code' in self.data.columns:
            stratify_col = self.data['region_code']
        else:
            stratify_col = None
        
        # ì²« ë²ˆì§¸ ë¶„í• : í›ˆë ¨+ê²€ì¦ vs í…ŒìŠ¤íŠ¸
        train_val_data, test_data = train_test_split(
            self.data, 
            test_size=test_size, 
            stratify=stratify_col,
            random_state=42
        )
        
        # ë‘ ë²ˆì§¸ ë¶„í• : í›ˆë ¨ vs ê²€ì¦
        val_ratio = validation_size / (1 - test_size)
        train_data, val_data = train_test_split(
            train_val_data,
            test_size=val_ratio,
            stratify=train_val_data['region_code'] if 'region_code' in train_val_data.columns else None,
            random_state=42
        )
        
        print(f"ğŸ“‹ ë¶„í•  ê²°ê³¼:")
        print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_data):,}ê°œ ({len(train_data)/len(self.data)*100:.1f}%)")
        print(f"   ê²€ì¦ ë°ì´í„°: {len(val_data):,}ê°œ ({len(val_data)/len(self.data)*100:.1f}%)")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data):,}ê°œ ({len(test_data)/len(self.data)*100:.1f}%)")
        
        # ì§€ì—­ë³„ ë¶„í¬ í™•ì¸
        if 'region' in self.data.columns:
            print(f"\nğŸŒ ì§€ì—­ë³„ ë¶„í•  í™•ì¸:")
            for dataset_name, dataset in [('í›ˆë ¨', train_data), ('ê²€ì¦', val_data), ('í…ŒìŠ¤íŠ¸', test_data)]:
                region_dist = dataset['region'].value_counts()
                print(f"   {dataset_name}: {dict(region_dist)}")
        
        return train_data, val_data, test_data
    
    def save_preprocessed_data(self):
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        """
        print("\nğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥")
        print("="*50)
        
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ì™„ì „ ì „ì²˜ë¦¬ëœ ë°ì´í„°
        full_filename = f"earthquake_preprocessed_full_{timestamp}.csv"
        self.data.to_csv(full_filename, index=False)
        print(f"ğŸ“ ì „ì²´ ì „ì²˜ë¦¬ ë°ì´í„°: {full_filename}")
        
        # 2. ConvLSTMìš© íŠ¹ì„± í–‰ë ¬
        if hasattr(self, 'feature_matrix_scaled'):
            features_filename = f"earthquake_features_convlstm_{timestamp}.csv"
            self.feature_matrix_scaled.to_csv(features_filename, index=False)
            print(f"ğŸ“ ConvLSTM íŠ¹ì„± í–‰ë ¬: {features_filename}")
        
        # 3. ê¸°ì¡´ í”„ë¡œì íŠ¸ í˜¸í™˜ ë°ì´í„°
        project_data = self.prepare_for_existing_project()
        project_filename = f"earthquake_project_compatible_{timestamp}.csv"
        project_data.to_csv(project_filename, index=False)
        print(f"ğŸ“ í”„ë¡œì íŠ¸ í˜¸í™˜ ë°ì´í„°: {project_filename}")
        
        # 4. ë°ì´í„° ë¶„í• 
        train_data, val_data, test_data = self.create_train_test_split()
        
        train_filename = f"earthquake_train_{timestamp}.csv"
        val_filename = f"earthquake_val_{timestamp}.csv"
        test_filename = f"earthquake_test_{timestamp}.csv"
        
        train_data.to_csv(train_filename, index=False)
        val_data.to_csv(val_filename, index=False)
        test_data.to_csv(test_filename, index=False)
        
        print(f"ğŸ“ í›ˆë ¨ ë°ì´í„°: {train_filename}")
        print(f"ğŸ“ ê²€ì¦ ë°ì´í„°: {val_filename}")
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_filename}")
        
        # 5. ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì €ì¥
        import json
        metadata = {
            'preprocessing_info': {
                'timestamp': timestamp,
                'original_records': len(self.data),
                'final_features': len(self.data.columns),
                'label_encoders': {k: v.classes_.tolist() for k, v in self.label_encoders.items()}
            },
            'file_mapping': {
                'full_data': full_filename,
                'convlstm_features': features_filename if hasattr(self, 'feature_matrix_scaled') else None,
                'project_compatible': project_filename,
                'train_data': train_filename,
                'validation_data': val_filename,
                'test_data': test_filename
            },
            'data_statistics': {
                'magnitude_range': f"{self.data['mag'].min():.1f}-{self.data['mag'].max():.1f}",
                'regions': self.data['region'].value_counts().to_dict() if 'region' in self.data.columns else {},
                'time_range': f"{self.data['year'].min()}-{self.data['year'].max()}" if 'year' in self.data.columns else "Unknown"
            }
        }
        
        metadata_filename = f"preprocessing_metadata_{timestamp}.json"
        with open(metadata_filename, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“‹ ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°: {metadata_filename}")
        
        return {
            'full_data': full_filename,
            'project_compatible': project_filename,
            'train': train_filename,
            'validation': val_filename,
            'test': test_filename,
            'metadata': metadata_filename
        }

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    print("ğŸš€ í–¥ìƒëœ ì§€ì§„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print("="*60)
    
    preprocessor = EnhancedEarthquakePreprocessor()
    
    # 1. ë°ì´í„° ë¡œë“œ
    data = preprocessor.load_integrated_dataset()
    if data is None:
        return
    
    # 2. íŠ¹ì„± ë¶„í¬ ë¶„ì„
    numeric_cols, categorical_cols = preprocessor.analyze_feature_distribution()
    
    # 3. ê³ ê¸‰ íŠ¹ì„± ìƒì„±
    enhanced_data = preprocessor.create_advanced_features()
    
    # 4. ê²°ì¸¡ê°’ ì²˜ë¦¬
    clean_data = preprocessor.handle_missing_values()
    
    # 5. ë²”ì£¼í˜• íŠ¹ì„± ì¸ì½”ë”©
    encoded_data = preprocessor.encode_categorical_features()
    
    # 6. ConvLSTMìš© íŠ¹ì„± ìƒì„±
    feature_matrix, core_features = preprocessor.create_convlstm_features()
    
    # 7. ë°ì´í„° ì €ì¥
    saved_files = preprocessor.save_preprocessed_data()
    
    print(f"\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for purpose, filename in saved_files.items():
        if filename:
            print(f"   - {purpose}: {filename}")
    
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   ìµœì¢… íŠ¹ì„±: {len(enhanced_data.columns):,}ê°œ")
    print(f"   ConvLSTM í•µì‹¬ íŠ¹ì„±: {len(core_features)}ê°œ")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. {saved_files['project_compatible']}ë¡œ ê¸°ì¡´ ConvLSTM ëª¨ë¸ ì¬í•™ìŠµ")
    print(f"   2. ì§€ì—­ë³„ êµì°¨ ê²€ì¦ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€")
    print(f"   3. ìƒˆë¡œìš´ íŠ¹ì„±ë“¤ë¡œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ í™•ì¸")
    print(f"   4. ì˜¤ê²½ë³´ìœ¨ 11% â†’ 5% ì´í•˜ ëª©í‘œ ë‹¬ì„±")

if __name__ == "__main__":
    main()