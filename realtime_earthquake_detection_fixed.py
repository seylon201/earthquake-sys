import time
from datetime import datetime, timedelta
import requests
from influxdb_client import InfluxDBClient
from urllib.parse import quote
import os
import csv
import pandas as pd
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (protobuf ë¬¸ì œ í•´ê²°)
os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# TensorFlow ì•ˆì „í•œ ë¡œë”©
def safe_load_tensorflow():
    """TensorFlowë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜"""
    try:
        import warnings
        warnings.filterwarnings('ignore')
        
        # TensorFlow ë¡œë”© ì‹œë„
        import tensorflow as tf
        tf.get_logger().setLevel('ERROR')
        
        # ë²„ì „ ì²´í¬
        tf_version = tf.__version__
        print(f"âœ… TensorFlow ë²„ì „: {tf_version}")
        
        from tensorflow.keras.models import load_model
        return tf, load_model, True
        
    except Exception as e:
        print(f"âŒ TensorFlow ë¡œë”© ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. pip install tensorflow==2.13.0")
        print("   2. pip install protobuf==3.20.3")
        print("   3. ì‹œìŠ¤í…œ ì¬ì‹œì‘ í›„ ì¬ì‹œë„")
        return None, None, False

# TensorFlow ë¡œë”© ì‹œë„
tf, load_model, tf_available = safe_load_tensorflow()

# âœ… 3í´ë˜ìŠ¤ ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = 'convlstm_3class_model.h5'

# âœ… ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_SAVE_DIR = "C:/earthquake_modeling/earthquake_project_v3/influxLogs"
RAW_DATA_DIR = os.path.join(BASE_SAVE_DIR, "base")  # ì›ì‹œ ë°ì´í„°
PROCESSED_DATA_DIR = os.path.join(BASE_SAVE_DIR, "processed")  # AI ì²˜ë¦¬ ê²°ê³¼
os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

# âœ… 3í´ë˜ìŠ¤ ì •ì˜
CLASS_NAMES = {0: 'ì§€ì§„', 1: 'ë¶ˆê·œì¹™ìƒí™œ', 2: 'ëª¨í„°ì§„ë™'}
CLASS_COLORS = {0: 'ğŸ”´', 1: 'ğŸŸ¢', 2: 'ğŸŸ '}

# ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì €ê° ì„¤ì • (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
EARTHQUAKE_CONFIDENCE_THRESHOLD = 0.88  # ì§€ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ìµœì†Œ ì‹ ë¢°ë„ (88%)
ENABLE_MULTI_CRITERIA_FILTER = True      # ë‹¤ì¤‘ ì¡°ê±´ í•„í„° ì‚¬ìš© ì—¬ë¶€
FALSE_POSITIVE_LOG = True                # ì˜¤ê²½ë³´ ì–µì œ ë¡œê·¸ ì¶œë ¥
ENABLE_ADVANCED_FILTERING = True         # ê³ ê¸‰ í•„í„°ë§ í™œì„±í™”

# ğŸ”§ ê³ ê¸‰ í•„í„° ì„ê³„ê°’ë“¤
ADVANCED_FILTER_THRESHOLDS = {
    'duration_ratio': 0.25,          # ì§€ì†ì„± ìµœì†Œ ë¹„ìœ¨ (25%)
    'max_frequency': 18,             # ìµœëŒ€ í—ˆìš© ì£¼íŒŒìˆ˜ (18Hz)
    'energy_balance_max': 0.65,      # ì—ë„ˆì§€ ë¶ˆê· í˜• ìµœëŒ€ê°’ (65%)
    'max_change_rate': 10,           # ìµœëŒ€ ë³€í™”ìœ¨
    'confidence_gap_min': 0.20,      # ìµœì†Œ ì‹ ë¢°ë„ ì°¨ì´ (20%)
    'consecutive_checks': 1          # ì—°ì† ê²€ì¦ íšŸìˆ˜
}

# ëª¨ë¸ ë¡œë”© (ì•ˆì „í•˜ê²Œ)
convlstm_model = None
if tf_available and load_model:
    print("ğŸ”„ 3í´ë˜ìŠ¤ ConvLSTM ëª¨ë¸ ë¡œë”© ì¤‘...")
    try:
        convlstm_model = load_model(MODEL_PATH)
        print("âœ… 3í´ë˜ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print(f"ğŸ” ëª¨ë¸ ì…ë ¥ í˜•íƒœ: {convlstm_model.input_shape}")
        print(f"ğŸ” ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {convlstm_model.output_shape}")
        print(f"ğŸ›¡ï¸ ì§€ì§„ ì‹ ë¢°ë„ ì„ê³„ê°’: {EARTHQUAKE_CONFIDENCE_THRESHOLD*100:.0f}%")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("âš ï¸ AI ì˜ˆì¸¡ ì—†ì´ ë°ì´í„° ìˆ˜ì§‘ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        convlstm_model = None
else:
    print("âš ï¸ TensorFlowë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ AI ì˜ˆì¸¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

# âœ… InfluxDB ì„¤ì •
INFLUX_TOKEN = "ZyegXlVhIdA26zFakbWjgVX863_pAtfXJPfsLGlA0wtfTxl7BHZJlMNLT5HHudXk58VzVScGnugA36w_buC4Zg=="
INFLUX_ORG = "kds"
INFLUX_BUCKET = "Lasung_3sensor of Max"
INFLUX_URL = "http://118.129.145.82:8086"
PORTS = [6060, 7001, 7053, 7060, 7070, 8010, 8080]
CHECK_INTERVAL = 1

# âœ… Node-RED ì„¤ì •
NODERED_BASE_URL = "http://118.129.145.82:8081/nodered/1min_event_lasung"

client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
query_api = client.query_api()

# âœ… ===== ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ë“¤ =====

def flatten_row(row):
    """ê¸°ì¡´ ì½”ë“œì˜ ë°ì´í„° í‰ë©´í™” í•¨ìˆ˜ (3ì„¼ì„œ ì§€ì›)"""
    flat = {
        "timestamp": "'" + row.get("timestamp"),
        "counter": row.get("counter")
    }
    for i in range(1, 4):  # sensor_1, sensor_2, sensor_3
        sensor_key = f"sensor_{i}"
        sensor_data = row.get(sensor_key, {})
        for key, value in sensor_data.items():
            flat[f"{sensor_key}_{key}"] = value
    return flat

def find_trigger_point(data, threshold=3.0, min_ratio=0.25):
    """í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ íƒì§€"""
    # 3ì¶• í•©ì„± ì§„ë„ ê³„ì‚°
    magnitude = np.sqrt(np.sum(data**2, axis=1))
    
    # 1ì´ˆ ìœˆë„ìš°(100ìƒ˜í”Œ)ë¡œ ìŠ¤ìº”
    for i in range(0, len(magnitude) - 100, 100):
        window = magnitude[i:i+100]
        high_intensity_count = np.sum(window >= threshold)
        
        if high_intensity_count >= (100 * min_ratio):  # 25ê°œ ì´ìƒ
            return i + 50  # ìœˆë„ìš° ì¤‘ì•™ì ì„ íŠ¸ë¦¬ê±°ë¡œ ì‚¬ìš©
    
    return None

# ğŸ›¡ï¸ ìƒˆë¡œìš´ ì§„ë™ íŠ¹ì„± ë¶„ì„ í•¨ìˆ˜ (ì˜¤ê²½ë³´ ì €ê°ìš©)
def analyze_vibration_characteristics(raw_data):
    """ì§„ë™ íŠ¹ì„± ë¶„ì„ìœ¼ë¡œ ì˜¤ê²½ë³´ ì €ê° ì§€ì›"""
    try:
        # 3ì¶• ë°ì´í„° ë¶„ë¦¬
        x_data = raw_data[:, 0]
        y_data = raw_data[:, 1] 
        z_data = raw_data[:, 2]
        
        # í•©ì„± ì§„ë„ ê³„ì‚°
        magnitude = np.sqrt(x_data**2 + y_data**2 + z_data**2)
        
        # 1. ì§„ë™ ì§€ì†ì„± ë¶„ì„ (ì „ì²´ êµ¬ê°„ ì¤‘ ê³ ì§„ë™ ë¹„ìœ¨)
        high_intensity_mask = magnitude >= 3.0
        duration_ratio = np.sum(high_intensity_mask) / len(magnitude)
        
        # 2. ì£¼íŒŒìˆ˜ ë¶„ì„ (FFT)
        fft = np.fft.fft(magnitude)
        freqs = np.fft.fftfreq(len(magnitude), d=0.01)  # 100Hz ìƒ˜í”Œë§
        dominant_freq_idx = np.argmax(np.abs(fft[1:len(fft)//2])) + 1
        dominant_freq = abs(freqs[dominant_freq_idx])
        
        # 3. ì¶•ê°„ ì—ë„ˆì§€ ë¶„í¬ ë¶„ì„
        x_energy = np.sum(x_data**2)
        y_energy = np.sum(y_data**2)
        z_energy = np.sum(z_data**2)
        total_energy = x_energy + y_energy + z_energy
        
        if total_energy > 0:
            x_ratio = x_energy / total_energy
            y_ratio = y_energy / total_energy
            z_ratio = z_energy / total_energy
            energy_balance = max(x_ratio, y_ratio, z_ratio)  # í•œ ì¶•ì— ì—ë„ˆì§€ê°€ ì§‘ì¤‘ë˜ëŠ” ì •ë„
        else:
            energy_balance = 0.33
        
        # 4. ê¸‰ê²©í•œ ë³€í™”ìœ¨ ë¶„ì„ (ì¶©ê²©ì„± ì§„ë™ ê°ì§€)
        x_diff = np.diff(x_data)
        y_diff = np.diff(y_data)
        z_diff = np.diff(z_data)
        max_change_rate = max(np.max(np.abs(x_diff)), np.max(np.abs(y_diff)), np.max(np.abs(z_diff)))
        
        # 5. ì§„ë™ íŒ¨í„´ì˜ ê·œì¹™ì„± ë¶„ì„
        if len(x_data) > 200:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
            x_autocorr = np.correlate(x_data, x_data, mode='full')
            autocorr_peak = np.max(x_autocorr[len(x_autocorr)//2+100:]) / np.max(x_autocorr)
        else:
            autocorr_peak = 0.5
        
        # 6. ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€ ë¶„ì„
        low_freq_energy = np.sum(np.abs(fft[freqs <= 10]))    # 10Hz ì´í•˜
        high_freq_energy = np.sum(np.abs(fft[freqs > 10]))    # 10Hz ì´ˆê³¼
        low_freq_dominance = low_freq_energy / (low_freq_energy + high_freq_energy) if (low_freq_energy + high_freq_energy) > 0 else 0.5
        
        characteristics = {
            'duration_ratio': duration_ratio,           # ê³ ì§„ë™ ì§€ì† ë¹„ìœ¨
            'dominant_frequency': abs(dominant_freq),   # ì£¼ìš” ì£¼íŒŒìˆ˜
            'energy_balance': energy_balance,           # ì—ë„ˆì§€ ë¶ˆê· í˜• ì •ë„
            'max_change_rate': max_change_rate,         # ìµœëŒ€ ë³€í™”ìœ¨
            'autocorr_peak': autocorr_peak,            # ìê¸°ìƒê´€ í”¼í¬ (ê·œì¹™ì„±)
            'max_amplitude': np.max(magnitude),         # ìµœëŒ€ ì§„í­
            'mean_amplitude': np.mean(magnitude),       # í‰ê·  ì§„í­
            'std_amplitude': np.std(magnitude),         # ì§„í­ í‘œì¤€í¸ì°¨
            'low_freq_dominance': low_freq_dominance,   # ì €ì£¼íŒŒìˆ˜ ìš°ì„¸ì„±
            'x_energy_ratio': x_energy / total_energy if total_energy > 0 else 0.33,
            'y_energy_ratio': y_energy / total_energy if total_energy > 0 else 0.33,
            'z_energy_ratio': z_energy / total_energy if total_energy > 0 else 0.33
        }
        
        return characteristics
        
    except Exception as e:
        print(f"âŒ ì§„ë™ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

# ğŸ›¡ï¸ í–¥ìƒëœ ì§€ì§„ ê°ì§€ ë¡œì§ (í•µì‹¬ ì˜¤ê²½ë³´ ì €ê° í•¨ìˆ˜)
def enhanced_earthquake_detection_logic(predictions, characteristics):
    """í–¥ìƒëœ ì§€ì§„ ê°ì§€ ë¡œì§ - ë‹¤ì¤‘ ê¸°ì¤€ ì ìš©í•˜ì—¬ ì˜¤ê²½ë³´ ì €ê°"""
    predicted_class = int(np.argmax(predictions[0]))
    confidence = float(predictions[0][predicted_class])
    
    # ê¸°ë³¸ ì˜ˆì¸¡ì´ ì§€ì§„ì´ ì•„ë‹Œ ê²½ìš° - ë°”ë¡œ í—ˆìš©
    if predicted_class != 0:
        return predicted_class, confidence, False, f"ëª¨ë¸ ì˜ˆì¸¡: {CLASS_NAMES[predicted_class]} (ì‹ ë¢°ë„: {confidence:.3f})"
    
    # ì§€ì§„ìœ¼ë¡œ ì˜ˆì¸¡ëœ ê²½ìš° ì¶”ê°€ ê²€ì¦ ì‹œì‘
    suppression_reasons = []
    pass_reasons = []
    
    # 1. ì‹ ë¢°ë„ ê²€ì‚¬ (ê°€ì¥ ì¤‘ìš”í•œ í•„í„°)
    if confidence < EARTHQUAKE_CONFIDENCE_THRESHOLD:
        suppression_reasons.append(f"ì‹ ë¢°ë„ ë¶€ì¡±: {confidence:.3f} < {EARTHQUAKE_CONFIDENCE_THRESHOLD}")
        if FALSE_POSITIVE_LOG:
            print(f"ğŸ›¡ï¸ 1ì°¨ í•„í„°: {suppression_reasons[0]}")
        return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[0]}"
    pass_reasons.append(f"ë†’ì€ ì‹ ë¢°ë„: {confidence:.3f}")
    
    # 2. ë‹¤ì¤‘ ì¡°ê±´ í•„í„° (í™œì„±í™”ëœ ê²½ìš°ë§Œ)
    if ENABLE_MULTI_CRITERIA_FILTER and characteristics:
        
        # ì§€ì†ì„± ê²€ì‚¬ (ì§€ì§„ì€ ë³´í†µ ë” ì˜¤ë˜ ì§€ì†)
        if characteristics['duration_ratio'] < ADVANCED_FILTER_THRESHOLDS['duration_ratio']:
            suppression_reasons.append(f"ì§„ë™ ì§€ì†ì„± ë¶€ì¡±: {characteristics['duration_ratio']:.3f}")
            if FALSE_POSITIVE_LOG:
                print(f"ğŸ›¡ï¸ ì§€ì†ì„± í•„í„°: {suppression_reasons[-1]}")
            return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[-1]}"
        pass_reasons.append(f"ì¶©ë¶„í•œ ì§€ì†ì„±: {characteristics['duration_ratio']:.3f}")
        
        # ì£¼íŒŒìˆ˜ ê²€ì‚¬ (ìƒí™œì§„ë™ì€ ë³´í†µ ê³ ì£¼íŒŒìˆ˜)
        if characteristics['dominant_frequency'] > ADVANCED_FILTER_THRESHOLDS['max_frequency']:
            suppression_reasons.append(f"ì£¼íŒŒìˆ˜ê°€ ë†’ìŒ: {characteristics['dominant_frequency']:.1f}Hz (ìƒí™œì§„ë™ ì˜ì‹¬)")
            if FALSE_POSITIVE_LOG:
                print(f"ğŸ›¡ï¸ ì£¼íŒŒìˆ˜ í•„í„°: {suppression_reasons[-1]}")
            return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[-1]}"
        pass_reasons.append(f"ì ì ˆí•œ ì£¼íŒŒìˆ˜: {characteristics['dominant_frequency']:.1f}Hz")
        
        # ì—ë„ˆì§€ ê· í˜• ê²€ì‚¬ (í•œ ì¶•ì—ë§Œ ì§‘ì¤‘ëœ ì—ë„ˆì§€ëŠ” ì¶©ê²©ì„± ì§„ë™)
        if characteristics['energy_balance'] > ADVANCED_FILTER_THRESHOLDS['energy_balance_max']:
            suppression_reasons.append(f"ì—ë„ˆì§€ ë¶ˆê· í˜•: {characteristics['energy_balance']:.3f} (ì¶©ê²©ì„± ì§„ë™ ì˜ì‹¬)")
            if FALSE_POSITIVE_LOG:
                print(f"ğŸ›¡ï¸ ì—ë„ˆì§€ í•„í„°: {suppression_reasons[-1]}")
            return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[-1]}"
        pass_reasons.append(f"ê· ë“±í•œ ì—ë„ˆì§€ ë¶„í¬: {characteristics['energy_balance']:.3f}")
        
        # ê¸‰ê²©í•œ ë³€í™”ìœ¨ ê²€ì‚¬ (ë„ˆë¬´ ê¸‰ê²©í•œ ë³€í™”ëŠ” ì¶©ê²©ì„±)
        if characteristics['max_change_rate'] > ADVANCED_FILTER_THRESHOLDS['max_change_rate']:
            suppression_reasons.append(f"ê¸‰ê²©í•œ ë³€í™”: {characteristics['max_change_rate']:.3f} (ì¶©ê²©ì„± ì§„ë™ ì˜ì‹¬)")
            if FALSE_POSITIVE_LOG:
                print(f"ğŸ›¡ï¸ ë³€í™”ìœ¨ í•„í„°: {suppression_reasons[-1]}")
            return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[-1]}"
        pass_reasons.append(f"ì™„ë§Œí•œ ë³€í™”: {characteristics['max_change_rate']:.3f}")
        
        # ğŸ†• ì €ì£¼íŒŒìˆ˜ ìš°ì„¸ì„± ê²€ì‚¬ (ì§€ì§„ì€ ì €ì£¼íŒŒê°€ ìš°ì„¸)
        if ENABLE_ADVANCED_FILTERING and characteristics['low_freq_dominance'] < 0.4:  # 40% ë¯¸ë§Œì´ë©´ ì˜ì‹¬
            suppression_reasons.append(f"ê³ ì£¼íŒŒ ìš°ì„¸: {characteristics['low_freq_dominance']:.3f} (ìƒí™œì§„ë™ ì˜ì‹¬)")
            if FALSE_POSITIVE_LOG:
                print(f"ğŸ›¡ï¸ ì£¼íŒŒìˆ˜ ìš°ì„¸ì„± í•„í„°: {suppression_reasons[-1]}")
            return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[-1]}"
        pass_reasons.append(f"ì €ì£¼íŒŒ ìš°ì„¸: {characteristics['low_freq_dominance']:.3f}")
    
    # 3. ë‹¤ë¥¸ í´ë˜ìŠ¤ì™€ì˜ ì‹ ë¢°ë„ ì°¨ì´ ê²€ì‚¬
    other_confidences = [predictions[0][i] for i in range(3) if i != 0]
    max_other_confidence = max(other_confidences)
    confidence_gap = confidence - max_other_confidence
    
    if confidence_gap < ADVANCED_FILTER_THRESHOLDS['confidence_gap_min']:
        suppression_reasons.append(f"ë‹¤ë¥¸ í´ë˜ìŠ¤ì™€ ì‹ ë¢°ë„ ì°¨ì´ ë¶€ì¡±: {confidence_gap:.3f}")
        if FALSE_POSITIVE_LOG:
            print(f"ğŸ›¡ï¸ ì‹ ë¢°ë„ ì°¨ì´ í•„í„°: {suppression_reasons[-1]}")
        return 1, confidence, True, f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {suppression_reasons[-1]}"
    pass_reasons.append(f"ì¶©ë¶„í•œ ì‹ ë¢°ë„ ì°¨ì´: {confidence_gap:.3f}")
    
    # ëª¨ë“  ê²€ì¦ í†µê³¼ - ì§€ì§„ìœ¼ë¡œ í™•ì •
    if FALSE_POSITIVE_LOG:
        print(f"âœ… ëª¨ë“  í•„í„° í†µê³¼: ì§€ì§„ìœ¼ë¡œ í™•ì •")
    return 0, confidence, False, f"âœ… ì§€ì§„ í™•ì •: {' | '.join(pass_reasons)}"

def preprocess_for_convlstm(csv_path):
    """í•™ìŠµ ë°ì´í„°ì™€ ì™„ì „íˆ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš© (3ì„¼ì„œ ì§€ì›)"""
    try:
        df = pd.read_csv(csv_path)
        
        # ì„¼ì„œ ì„ íƒ (sensor_1 ìš°ì„ , ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì„¼ì„œ ì‚¬ìš©)
        sensor_cols = []
        for i in range(1, 4):
            x_col = f'sensor_{i}_x'
            y_col = f'sensor_{i}_y' 
            z_col = f'sensor_{i}_z'
            
            if all(col in df.columns for col in [x_col, y_col, z_col]):
                sensor_cols = [x_col, y_col, z_col]
                print(f"ğŸ“¡ ì‚¬ìš© ì„¼ì„œ: sensor_{i}")
                break
        
        if not sensor_cols:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¼ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None, None, "ì„¼ì„œ ë°ì´í„° ì—†ìŒ"
        
        # 3ì¶• ë°ì´í„° ì¶”ì¶œ
        sensor_x = df[sensor_cols[0]].astype(float).values
        sensor_y = df[sensor_cols[1]].astype(float).values
        sensor_z = df[sensor_cols[2]].astype(float).values
        
        # ë°ì´í„° ê²°í•©
        raw_data = np.stack([sensor_x, sensor_y, sensor_z], axis=1)
        print(f"ğŸ“Š ì›ì‹œ ë°ì´í„° í¬ê¸°: {raw_data.shape}")
        
        # ğŸ›¡ï¸ ì§„ë™ íŠ¹ì„± ë¶„ì„ (ì˜¤ê²½ë³´ ì €ê°ìš©)
        characteristics = analyze_vibration_characteristics(raw_data)
        
        # íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ íƒì§€
        trigger_idx = find_trigger_point(raw_data)
        
        if trigger_idx is None:
            print("âš ï¸ íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë°ì´í„° ì¤‘ì•™ ì‚¬ìš©")
            trigger_idx = len(raw_data) // 2
        else:
            print(f"ğŸ¯ íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ ë°œê²¬: {trigger_idx}ë²ˆì§¸ ìƒ˜í”Œ")
        
        # 40ì´ˆ ìŠ¬ë¼ì´ì‹± (í•™ìŠµê³¼ ë™ì¼)
        PRE_SAMPLES = 1500   # ì „ 15ì´ˆ
        POST_SAMPLES = 2500  # í›„ 25ì´ˆ
        TOTAL_SAMPLES = 4000 # ì´ 40ì´ˆ
        
        start_idx = max(0, trigger_idx - PRE_SAMPLES)
        end_idx = trigger_idx + POST_SAMPLES
        
        print(f"ğŸ“ ìŠ¬ë¼ì´ì‹± êµ¬ê°„: {start_idx} ~ {end_idx}")
        
        # ë°ì´í„° ì¶”ì¶œ ë° íŒ¨ë”©
        if end_idx <= len(raw_data):
            sliced_data = raw_data[start_idx:end_idx, :3]
        else:
            available_data = raw_data[start_idx:, :3]
            pad_length = TOTAL_SAMPLES - len(available_data)
            padding = np.zeros((pad_length, 3))
            sliced_data = np.vstack([available_data, padding])
            print(f"âš ï¸ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ {pad_length}ê°œ ìƒ˜í”Œ íŒ¨ë”© ì ìš©")
        
        if len(sliced_data) > TOTAL_SAMPLES:
            sliced_data = sliced_data[:TOTAL_SAMPLES]
        
        if len(sliced_data) != TOTAL_SAMPLES:
            pad_length = TOTAL_SAMPLES - len(sliced_data)
            padding = np.zeros((pad_length, 3))
            sliced_data = np.vstack([sliced_data, padding])
        
        print(f"âœ… ìŠ¬ë¼ì´ì‹± ì™„ë£Œ: {sliced_data.shape}")
        
        # ConvLSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
        reshaped = sliced_data.reshape(40, 100, 3)      # 40í”„ë ˆì„, ê° 100ìƒ˜í”Œ, 3ì¶•
        reshaped = np.transpose(reshaped, (0, 2, 1))    # (40, 3, 100)
        reshaped = np.expand_dims(reshaped, axis=-1)    # (40, 3, 100, 1)
        reshaped = np.expand_dims(reshaped, axis=0)     # (1, 40, 3, 100, 1)
        
        print(f"ğŸ”„ í˜•íƒœ ë³€í™˜ ì™„ë£Œ: {reshaped.shape}")
        
        # z-score ì •ê·œí™”
        mean = reshaped.mean()
        std = reshaped.std()
        
        if std > 0:
            normalized = (reshaped - mean) / std
            print(f"ğŸ“ ì •ê·œí™” ì™„ë£Œ: í‰ê· ={mean:.6f}, í‘œì¤€í¸ì°¨={std:.6f}")
        else:
            normalized = reshaped
            print("âš ï¸ í‘œì¤€í¸ì°¨ê°€ 0ì´ë¯€ë¡œ ì •ê·œí™” ìƒëµ")
        
        preprocess_info = {
            'original_length': len(raw_data),
            'trigger_point': trigger_idx,
            'slicing_range': (start_idx, end_idx),
            'final_shape': normalized.shape,
            'normalization': {'mean': float(mean), 'std': float(std)},
            'used_sensor': sensor_cols[0].split('_')[1],  # sensor_1 -> 1
            'characteristics': characteristics  # ğŸ›¡ï¸ ì§„ë™ íŠ¹ì„± ì¶”ê°€
        }
        
        return normalized, df, preprocess_info
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None, str(e)

def predict_and_save_result(raw_csv_path, processed_csv_path):
    """AI ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥ (ì˜¤ê²½ë³´ ì €ê° ë¡œì§ ì ìš©)"""
    if convlstm_model is None:
        print("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì˜ˆì¸¡ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return None
    
    try:
        print(f"\nğŸ”„ === AI ì˜ˆì¸¡ ì‹œì‘: {os.path.basename(raw_csv_path)} ===")
        
        # ì „ì²˜ë¦¬ (ì§„ë™ íŠ¹ì„± ë¶„ì„ í¬í•¨)
        X, df, preprocess_info = preprocess_for_convlstm(raw_csv_path)
        
        if X is None:
            print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {preprocess_info}")
            return None
        
        characteristics = preprocess_info.get('characteristics')
        
        # ğŸ›¡ï¸ ì§„ë™ íŠ¹ì„± ì¶œë ¥
        if characteristics and FALSE_POSITIVE_LOG:
            print(f"ğŸ” ì§„ë™ íŠ¹ì„± ë¶„ì„:")
            print(f"   - ì§€ì†ì„±: {characteristics['duration_ratio']:.3f}")
            print(f"   - ì£¼ìš” ì£¼íŒŒìˆ˜: {characteristics['dominant_frequency']:.1f}Hz")
            print(f"   - ì—ë„ˆì§€ ê· í˜•: {characteristics['energy_balance']:.3f}")
            print(f"   - ìµœëŒ€ ë³€í™”ìœ¨: {characteristics['max_change_rate']:.3f}")
            print(f"   - ìµœëŒ€ ì§„í­: {characteristics['max_amplitude']:.3f}")
            print(f"   - ì €ì£¼íŒŒ ìš°ì„¸ì„±: {characteristics['low_freq_dominance']:.3f}")
        
        # ëª¨ë¸ ì˜ˆì¸¡
        start_time = time.time()
        predictions = convlstm_model.predict(X, verbose=0)
        inference_time = time.time() - start_time
        
        # ğŸ›¡ï¸ í–¥ìƒëœ ì§€ì§„ ê°ì§€ ë¡œì§ ì ìš©
        final_class, final_confidence, is_suppressed, detection_reason = enhanced_earthquake_detection_logic(
            predictions, characteristics
        )
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ§  === ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ===")
        for i, (class_id, class_name) in enumerate(CLASS_NAMES.items()):
            prob = predictions[0][i]
            icon = CLASS_COLORS[class_id]
            print(f"   {icon} {class_name}: {prob:.6f} ({prob*100:.2f}%)")
        
        original_class = int(np.argmax(predictions[0]))
        original_confidence = float(predictions[0][original_class])
        
        print(f"\nğŸ¯ === ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ ===")
        print(f"ì›ë³¸ ì˜ˆì¸¡: {CLASS_NAMES[original_class]} (ì‹ ë¢°ë„: {original_confidence:.4f})")
        print(f"ìµœì¢… ë¶„ë¥˜: {CLASS_NAMES[final_class]} (ì‹ ë¢°ë„: {final_confidence:.4f})")
        print(f"ê²€ì¦ ê²°ê³¼: {detection_reason}")
        
        # ê²½ë³´ íŒë³„
        if final_class == 0:  # ì§€ì§„
            print(f"\nğŸš¨ === ì§€ì§„ ê°ì§€! ê²½ë³´ ë°œë ¹! ===")
            alert_status = "EARTHQUAKE_ALERT"
        else:
            if is_suppressed:
                print(f"\nğŸ›¡ï¸ === ì˜¤ê²½ë³´ ì–µì œë¨ - ê²½ë³´ ì°¨ë‹¨ ===")
                alert_status = "FALSE_POSITIVE_SUPPRESSED"
            else:
                print(f"\nâœ… === ë¹„ì§€ì§„ ì§„ë™ ê°ì§€ - ì •ìƒ ìƒíƒœ ===")
                alert_status = "NO_ALERT"
        
        # ì˜ˆì¸¡ ê²°ê³¼ê°€ í¬í•¨ëœ CSV ìƒì„±
        if df is not None:
            # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
            df['predicted_class'] = final_class
            df['predicted_class_name'] = CLASS_NAMES[final_class]
            df['confidence'] = final_confidence
            df['original_predicted_class'] = original_class  # ğŸ›¡ï¸ ì›ë³¸ ì˜ˆì¸¡ ë³´ì¡´
            df['original_confidence'] = original_confidence  # ğŸ›¡ï¸ ì›ë³¸ ì‹ ë¢°ë„ ë³´ì¡´
            df['is_false_positive_suppressed'] = is_suppressed  # ğŸ›¡ï¸ ì–µì œ ì—¬ë¶€
            df['inference_time'] = inference_time
            df['alert_status'] = alert_status
            
            # ì „ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
            df['trigger_point'] = preprocess_info['trigger_point']
            df['original_length'] = preprocess_info['original_length']
            df['used_sensor'] = preprocess_info['used_sensor']
            df['normalization_mean'] = preprocess_info['normalization']['mean']
            df['normalization_std'] = preprocess_info['normalization']['std']
            
            # ğŸ›¡ï¸ ì§„ë™ íŠ¹ì„± ì •ë³´ ì¶”ê°€
            if characteristics:
                for key, value in characteristics.items():
                    df[f'vibration_{key}'] = value
            
            # ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥  ì¶”ê°€
            for i, name in CLASS_NAMES.items():
                df[f'prob_{name}'] = predictions[0][i]
            
            # ğŸ›¡ï¸ í•„í„°ë§ í†µê³¼ ì—¬ë¶€ ì •ë³´ ì¶”ê°€
            df['earthquake_confidence_threshold'] = EARTHQUAKE_CONFIDENCE_THRESHOLD
            df['multi_criteria_filter_enabled'] = ENABLE_MULTI_CRITERIA_FILTER
            df['advanced_filtering_enabled'] = ENABLE_ADVANCED_FILTERING
            
            # ì²˜ë¦¬ëœ ê²°ê³¼ ì €ì¥
            df.to_csv(processed_csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ AI ì²˜ë¦¬ ê²°ê³¼ ì €ì¥: {os.path.basename(processed_csv_path)}")
        
        return {
            'predicted_class': final_class,
            'class_name': CLASS_NAMES[final_class],
            'confidence': final_confidence,
            'original_class': original_class,  # ğŸ›¡ï¸ ì›ë³¸ ì •ë³´ ë³´ì¡´
            'original_confidence': original_confidence,
            'is_false_positive_suppressed': is_suppressed,
            'alert_status': alert_status,
            'preprocess_info': preprocess_info,
            'all_probabilities': predictions[0].tolist(),
            'filter_applied': 'enhanced_multi_criteria'  # ğŸ›¡ï¸ ì ìš©ëœ í•„í„° ì •ë³´
        }
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

# ğŸ›¡ï¸ íŒŒì¼ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ í•¨ìˆ˜
def is_already_processed(raw_filename):
    """ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸"""
    ai_filename = raw_filename.replace("event_", "ai_")
    processed_path = f"{PROCESSED_DATA_DIR}/{ai_filename}"
    return os.path.exists(processed_path)

# ğŸ›¡ï¸ ì‹œìŠ¤í…œ ê±´ê°•ì„± ì²´í¬ í•¨ìˆ˜
def system_health_check():
    """ì‹œìŠ¤í…œ ê±´ê°•ì„± ì²´í¬"""
    health_status = {
        'tensorflow_available': tf_available,
        'model_loaded': convlstm_model is not None,
        'directories_ready': all([
            os.path.exists(RAW_DATA_DIR),
            os.path.exists(PROCESSED_DATA_DIR)
        ]),
        'influxdb_connected': False
    }
    
    # InfluxDB ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        test_query = f'from(bucket: "{INFLUX_BUCKET}") |> range(start: -1m) |> limit(n:1)'
        result = query_api.query(org=INFLUX_ORG, query=test_query)
        health_status['influxdb_connected'] = True
    except:
        health_status['influxdb_connected'] = False
    
    return health_status

# âœ… í†µê³„ ì¶”ì  (ì˜¤ê²½ë³´ ì €ê° í†µê³„ ì¶”ê°€)
detection_stats = {
    'total_events': 0,
    'class_counts': {name: 0 for name in CLASS_NAMES.values()},
    'alert_count': 0,
    'suppressed_count': 0,
    'false_positive_suppressed': 0,  # ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ íšŸìˆ˜
    'processing_times': [],  # ğŸ†• ì²˜ë¦¬ ì‹œê°„ ì¶”ì 
    'filter_effectiveness': {  # ğŸ†• í•„í„°ë³„ íš¨ê³¼ ì¶”ì 
        'confidence_filter': 0,
        'duration_filter': 0,
        'frequency_filter': 0,
        'energy_filter': 0,
        'change_rate_filter': 0,
        'confidence_gap_filter': 0,
        'low_freq_filter': 0
    },
    'start_time': datetime.now()
}

# ì‹œìŠ¤í…œ ì‹œì‘ ì •ë³´ ì¶œë ¥
print("\nğŸš€ === í–¥ìƒëœ 3ì„¼ì„œ ì§€ì› ì‹¤ì‹œê°„ ì§€ì§„ ê°ì§€ ì‹œìŠ¤í…œ ì‹œì‘! ===")
print(f"ğŸ“Š AI ë¶„ë¥˜ í´ë˜ìŠ¤: {list(CLASS_NAMES.values())}")
print(f"ğŸ” ê°ì‹œ ëŒ€ìƒ í¬íŠ¸: {PORTS}")
print(f"â±ï¸ ì²´í¬ ì£¼ê¸°: {CHECK_INTERVAL}ì´ˆ")
print(f"ğŸ“¡ ì§€ì› ì„¼ì„œ: sensor_1, sensor_2, sensor_3")
print(f"ğŸŒ Node-RED ì„œë²„: {NODERED_BASE_URL}")
print(f"ğŸ’¾ ì›ì‹œ ë°ì´í„° ì €ì¥: {RAW_DATA_DIR}")
print(f"ğŸ§  AI ì²˜ë¦¬ ê²°ê³¼: {PROCESSED_DATA_DIR}")
print(f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì €ê° ì„¤ì •:")
print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {EARTHQUAKE_CONFIDENCE_THRESHOLD*100:.0f}%")
print(f"   - ë‹¤ì¤‘ ì¡°ê±´ í•„í„°: {'í™œì„±í™”' if ENABLE_MULTI_CRITERIA_FILTER else 'ë¹„í™œì„±í™”'}")
print(f"   - ê³ ê¸‰ í•„í„°ë§: {'í™œì„±í™”' if ENABLE_ADVANCED_FILTERING else 'ë¹„í™œì„±í™”'}")

# ğŸ›¡ï¸ ì‹œìŠ¤í…œ ê±´ê°•ì„± ì²´í¬
health = system_health_check()
print(f"\nğŸ¥ ì‹œìŠ¤í…œ ê±´ê°•ì„± ì²´í¬:")
print(f"   - TensorFlow: {'âœ…' if health['tensorflow_available'] else 'âŒ'}")
print(f"   - ëª¨ë¸ ë¡œë”©: {'âœ…' if health['model_loaded'] else 'âŒ'}")
print(f"   - ë””ë ‰í† ë¦¬: {'âœ…' if health['directories_ready'] else 'âŒ'}")
print(f"   - InfluxDB: {'âœ…' if health['influxdb_connected'] else 'âŒ'}")

print("="*70)

# âœ… ì‹¤ì‹œê°„ ê°ì‹œ ë£¨í”„
try:
    consecutive_no_data = 0  # ì—°ì†ìœ¼ë¡œ ë°ì´í„°ê°€ ì—†ëŠ” íšŸìˆ˜
    
    while True:
        now = datetime.utcnow()
        start = now - timedelta(seconds=CHECK_INTERVAL)
        data_found = False

        for port in PORTS:
            query = f'''
            from(bucket: "{INFLUX_BUCKET}")
              |> range(start: {start.isoformat()}Z, stop: {now.isoformat()}Z)
              |> filter(fn: (r) => r._field == "intensity" and r._measurement == "{port}")
              |> sort(columns: ["_time"], desc: true)
              |> limit(n:1)
            '''

            result = query_api.query(org=INFLUX_ORG, query=query)

            for table in result:
                for record in table.records:
                    data_found = True
                    intensity = record.get_value()
                    if not isinstance(intensity, (int, float)):
                        continue

                    if intensity >= 3.0:
                        event_time = record.get_time().astimezone()
                        kst_time = event_time.strftime("%Y-%m-%d %H:%M:%S")
                        
                        print(f"\nğŸ”¥ === ì§„ë„ 3 ì´ìƒ ê°ì§€ ===")
                        print(f"ğŸ“… ì‹œê°„: {kst_time}")
                        print(f"ğŸŒ í¬íŠ¸: {port}")
                        print(f"ğŸ“Š ì§„ë„: {intensity:.2f}")
                        
                        # ğŸ›¡ï¸ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ ì²´í¬
                        estimated_filename = f"event_{port}_{kst_time.replace(':', '-').replace(' ', '_')}.csv"
                        if is_already_processed(estimated_filename):
                            print(f"âš ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ê±´ë„ˆëœ€: {estimated_filename}")
                            continue
                        
                        print("â³ 40ì´ˆ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘...")
                        
                        # ì§„í–‰ ìƒí™© í‘œì‹œ
                        for i in range(40, 0, -5):
                            print(f"   ğŸ• {i}ì´ˆ ë‚¨ìŒ...")
                            time.sleep(5)

                        encoded_time = quote(kst_time)
                        url = f"{NODERED_BASE_URL}/{encoded_time}/{port}"
                        print(f"ğŸ”— Node-RED í˜¸ì¶œ: {url}")

                        try:
                            res = requests.get(url, timeout=30)
                            if res.status_code == 200:
                                data = res.json()
                                
                                # ğŸ”§ Node-RED ì‘ë‹µì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                                filename = None
                                
                                # ë°©ë²• 1: ì‘ë‹µì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
                                if isinstance(data, dict):
                                    if "message" in data:
                                        message = str(data["message"])
                                        print(f"ğŸ” Node-RED ë©”ì‹œì§€: {message}")
                                        
                                        if "generated" in message and ".csv" in message:
                                            import re
                                            csv_match = re.search(r'event_\d+_[\d\-_]+\.csv', message)
                                            if csv_match:
                                                filename = csv_match.group()
                                                print(f"ğŸ“ ì¶”ì¶œëœ íŒŒì¼ëª…: {filename}")
                                    
                                    elif "filename" in data:
                                        filename = data["filename"]
                                        print(f"ğŸ“ ì§ì ‘ íŒŒì¼ëª…: {filename}")
                                
                                # ë°©ë²• 2: ì˜ˆìƒ íŒŒì¼ëª…ìœ¼ë¡œ ëŒ€ì²´
                                if not filename:
                                    formatted_time = kst_time.replace(':', '-').replace(' ', '_')
                                    filename = f"event_{port}_{formatted_time}.csv"
                                    print(f"ğŸ¯ ì˜ˆìƒ íŒŒì¼ëª… ì‚¬ìš©: {filename}")
                                
                                csv_path = f"{RAW_DATA_DIR}/{filename}"
                                
                                # íŒŒì¼ ì¡´ì¬ í™•ì¸ (ìµœëŒ€ 15ì´ˆ ëŒ€ê¸°)
                                max_wait = 15
                                wait_count = 0
                                while wait_count < max_wait and not os.path.exists(csv_path):
                                    print(f"â³ íŒŒì¼ ìƒì„± ëŒ€ê¸° ì¤‘... ({wait_count + 1}/{max_wait}ì´ˆ)")
                                    time.sleep(1)
                                    wait_count += 1
                                
                                if os.path.exists(csv_path):
                                    file_size = os.path.getsize(csv_path)
                                    print(f"âœ… íŒŒì¼ ë°œê²¬! í¬ê¸°: {file_size} bytes")
                                    
                                    # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ ì²´í¬
                                    ai_filename = filename.replace("event_", "ai_")
                                    processed_csv_path = f"{PROCESSED_DATA_DIR}/{ai_filename}"
                                    
                                    if os.path.exists(processed_csv_path):
                                        print(f"âš ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆëœ€: {ai_filename}")
                                        continue
                                    
                                    # ğŸ›¡ï¸ AI ì˜ˆì¸¡ ìˆ˜í–‰ (ì˜¤ê²½ë³´ ì €ê° ë¡œì§ ì ìš©)
                                    processing_start = time.time()
                                    ai_result = predict_and_save_result(csv_path, processed_csv_path)
                                    processing_time = time.time() - processing_start
                                    
                                    if ai_result:
                                        # ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì €ê° í†µê³„ ì—…ë°ì´íŠ¸
                                        detection_stats['total_events'] += 1
                                        detection_stats['class_counts'][ai_result['class_name']] += 1
                                        detection_stats['processing_times'].append(processing_time)
                                        
                                        if ai_result['alert_status'] == "EARTHQUAKE_ALERT":
                                            detection_stats['alert_count'] += 1
                                        elif ai_result['alert_status'] == "FALSE_POSITIVE_SUPPRESSED":
                                            detection_stats['suppressed_count'] += 1
                                            detection_stats['false_positive_suppressed'] += 1
                                        else:
                                            detection_stats['suppressed_count'] += 1
                                        
                                        # ğŸ›¡ï¸ í–¥ìƒëœ í†µê³„ ì¶œë ¥
                                        print(f"\nğŸ“ˆ === ì˜¤ê²½ë³´ ì €ê° ì„±ê³¼ ë¦¬í¬íŠ¸ ===")
                                        print(f"ğŸ“Š ì´ ì´ë²¤íŠ¸: {detection_stats['total_events']}ê±´")
                                        print(f"ğŸš¨ ì§€ì§„ ê²½ë³´: {detection_stats['alert_count']}ê±´")
                                        print(f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œ: {detection_stats['false_positive_suppressed']}ê±´")
                                        print(f"âœ… ì •ìƒ ì–µì œ: {detection_stats['suppressed_count'] - detection_stats['false_positive_suppressed']}ê±´")
                                        print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
                                        
                                        if detection_stats['total_events'] > 0:
                                            alert_rate = detection_stats['alert_count'] / detection_stats['total_events'] * 100
                                            false_positive_suppression_rate = detection_stats['false_positive_suppressed'] / detection_stats['total_events'] * 100
                                            total_suppression_rate = detection_stats['suppressed_count'] / detection_stats['total_events'] * 100
                                            avg_processing_time = np.mean(detection_stats['processing_times'])
                                            
                                            print(f"ğŸ“ˆ ì§€ì§„ ê²½ë³´ìœ¨: {alert_rate:.1f}%")
                                            print(f"ğŸ›¡ï¸ ì˜¤ê²½ë³´ ì–µì œìœ¨: {false_positive_suppression_rate:.1f}%")
                                            print(f"ğŸ“Š ì´ ì–µì œìœ¨: {total_suppression_rate:.1f}%")
                                            print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
                                            
                                            # ğŸ†• ì˜¤ê²½ë³´ ì €ê° íš¨ê³¼ ê³„ì‚°
                                            if detection_stats['false_positive_suppressed'] > 0:
                                                potential_false_alarms = detection_stats['alert_count'] + detection_stats['false_positive_suppressed']
                                                reduction_effectiveness = (detection_stats['false_positive_suppressed'] / potential_false_alarms) * 100
                                                print(f"ğŸ¯ ì˜¤ê²½ë³´ ì €ê° íš¨ê³¼: {reduction_effectiveness:.1f}%")
                                        
                                        print(f"\nğŸ“Š í´ë˜ìŠ¤ë³„ ë¶„í¬:")
                                        for class_name, count in detection_stats['class_counts'].items():
                                            if count > 0:
                                                icon = CLASS_COLORS[list(CLASS_NAMES.values()).index(class_name)]
                                                percentage = count / detection_stats['total_events'] * 100
                                                print(f"   {icon} {class_name}: {count}ê±´ ({percentage:.1f}%)")
                                                
                                        runtime = datetime.now() - detection_stats['start_time']
                                        print(f"ğŸ•’ ì´ ì‹¤í–‰ ì‹œê°„: {runtime}")
                                        print("="*50)
                                    else:
                                        print(f"âŒ AI ì˜ˆì¸¡ ì‹¤íŒ¨")
                                else:
                                    print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
                                    
                                    # ìµœì‹  íŒŒì¼ ì°¾ê¸° ì‹œë„
                                    try:
                                        csv_files = [f for f in os.listdir(RAW_DATA_DIR) 
                                                   if f.startswith(f"event_{port}_") and f.endswith('.csv')]
                                        if csv_files:
                                            latest_file = max(csv_files, 
                                                            key=lambda x: os.path.getmtime(os.path.join(RAW_DATA_DIR, x)))
                                            print(f"ğŸ” ìµœì‹  íŒŒì¼ ì‹œë„: {latest_file}")
                                            csv_path = f"{RAW_DATA_DIR}/{latest_file}"
                                            
                                            if os.path.exists(csv_path):
                                                print(f"âœ… ìµœì‹  íŒŒì¼ ì‚¬ìš©: {latest_file}")
                                                # AI ì²˜ë¦¬ ì¬ì‹œë„...
                                    except Exception as e:
                                        print(f"âŒ ìµœì‹  íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {e}")
                            else:
                                print(f"âŒ Node-RED ì‘ë‹µ ì˜¤ë¥˜: {res.status_code}")
                                if hasattr(res, 'text'):
                                    print(f"   ì‘ë‹µ: {res.text[:200]}")
                                    
                        except requests.exceptions.Timeout:
                            print(f"âŒ Node-RED ìš”ì²­ íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)")
                        except Exception as e:
                            print(f"âŒ Node-RED ìš”ì²­ ì‹¤íŒ¨: {e}")
                            import traceback
                            traceback.print_exc()
        
        # ğŸ›¡ï¸ ì—°ì† ë¬´ë°ì´í„° ëª¨ë‹ˆí„°ë§
        if not data_found:
            consecutive_no_data += 1
            if consecutive_no_data % 300 == 0:  # 5ë¶„ë§ˆë‹¤ ì•Œë¦¼
                print(f"â° ë°ì´í„° ì—†ìŒ ì§€ì† ì¤‘... ({consecutive_no_data}ì´ˆ)")
        else:
            consecutive_no_data = 0

        time.sleep(CHECK_INTERVAL)

except KeyboardInterrupt:
    print("\n\nğŸ›‘ === ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨ ===")
    print("ğŸ“Š ìµœì¢… ì˜¤ê²½ë³´ ì €ê° ì„±ê³¼ ë¦¬í¬íŠ¸:")
    print(f"   ì´ ì´ë²¤íŠ¸: {detection_stats['total_events']}ê±´")
    print(f"   ì§€ì§„ ê²½ë³´: {detection_stats['alert_count']}ê±´") 
    print(f"   ì˜¤ê²½ë³´ ì–µì œ: {detection_stats['false_positive_suppressed']}ê±´")
    print(f"   ì •ìƒ ì–µì œ: {detection_stats['suppressed_count'] - detection_stats['false_positive_suppressed']}ê±´")
    
    if detection_stats['total_events'] > 0:
        alert_rate = detection_stats['alert_count'] / detection_stats['total_events'] * 100
        false_positive_suppression_rate = detection_stats['false_positive_suppressed'] / detection_stats['total_events'] * 100
        total_suppression_rate = detection_stats['suppressed_count'] / detection_stats['total_events'] * 100
        avg_processing_time = np.mean(detection_stats['processing_times']) if detection_stats['processing_times'] else 0
        
        print(f"   ì§€ì§„ ê²½ë³´ìœ¨: {alert_rate:.1f}%")
        print(f"   ì˜¤ê²½ë³´ ì–µì œìœ¨: {false_positive_suppression_rate:.1f}%")
        print(f"   ì´ ì–µì œìœ¨: {total_suppression_rate:.1f}%")
        print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
        
        # ğŸ›¡ï¸ ìµœì¢… ì˜¤ê²½ë³´ ì €ê° íš¨ê³¼ ë¶„ì„
        if detection_stats['false_positive_suppressed'] > 0:
            potential_false_alarms = detection_stats['alert_count'] + detection_stats['false_positive_suppressed']
            reduction_effectiveness = (detection_stats['false_positive_suppressed'] / potential_false_alarms) * 100
            original_false_alarm_rate = (potential_false_alarms / detection_stats['total_events']) * 100
            final_false_alarm_rate = alert_rate
            improvement = original_false_alarm_rate - final_false_alarm_rate
            
            print(f"\nğŸ¯ === ì˜¤ê²½ë³´ ì €ê° íš¨ê³¼ ë¶„ì„ ===")
            print(f"   ì›ë˜ ì˜ˆìƒ ì˜¤ê²½ë³´ìœ¨: {original_false_alarm_rate:.1f}%")
            print(f"   ìµœì¢… ì‹¤ì œ ê²½ë³´ìœ¨: {final_false_alarm_rate:.1f}%")
            print(f"   ê°œì„  íš¨ê³¼: {improvement:.1f}%p ê°ì†Œ")
            print(f"   ì €ê° íš¨ê³¼: {reduction_effectiveness:.1f}%")
    
    print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ìµœì¢… ë¶„í¬:")
    for class_name, count in detection_stats['class_counts'].items():
        if count > 0:
            icon = CLASS_COLORS[list(CLASS_NAMES.values()).index(class_name)]
            percentage = count / detection_stats['total_events'] * 100 if detection_stats['total_events'] > 0 else 0
            print(f"   {icon} {class_name}: {count}ê±´ ({percentage:.1f}%)")
    
    total_runtime = datetime.now() - detection_stats['start_time']
    print(f"\nğŸ•’ ì´ ì‹¤í–‰ ì‹œê°„: {total_runtime}")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜:")
    print(f"   ì›ì‹œ ë°ì´í„°: {RAW_DATA_DIR}")
    print(f"   AI ì²˜ë¦¬ ê²°ê³¼: {PROCESSED_DATA_DIR}")
    print("\nğŸ‰ í–¥ìƒëœ ì˜¤ê²½ë³´ ì €ê° ì§€ì§„ ê°ì§€ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ‘‹ ì‹œìŠ¤í…œ ì¢…ë£Œ")

except Exception as e:
    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()
    print(f"\nğŸ’¾ ì˜¤ë¥˜ ë°œìƒ ì‹œì  í†µê³„:")
    print(f"   ì²˜ë¦¬ëœ ì´ë²¤íŠ¸: {detection_stats['total_events']}ê±´")
    print(f"   ì‹¤í–‰ ì‹œê°„: {datetime.now() - detection_stats['start_time']}")
    print("ğŸ”§ ì‹œìŠ¤í…œ ì¬ì‹œì‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")